#!/usr/bin/env python3
"""
íšŒì‚¬ ë¶„ì„ MCP ì„œë²„ (Company Analysis MCP Server)

ê¸°ì—… ì •ë³´ ê²€ìƒ‰, SWOT ë¶„ì„, ë„¤ì´ë²„ ê²€ìƒ‰, êµ¬ê¸€ ê²€ìƒ‰ ë“±ì˜ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” í†µí•© MCP ì„œë²„ì…ë‹ˆë‹¤.
ì•„ë˜ íŒŒì¼ë“¤ì˜ ê¸°ëŠ¥ì„ í†µí•©:
- dart.py - ê¸ˆìœµê°ë…ì› ê¸°ì—…ê³µì‹œ ê²€ìƒ‰
- swot_analysis.py - ê¸°ì—… SWOT ë¶„ì„
- py-mcp-naver-search/server.py - ë„¤ì´ë²„ ê²€ìƒ‰
- google-search-mcp-server/google_search_mcp_server.py - êµ¬ê¸€ ê²€ìƒ‰
"""

# === ê³µí†µ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ===
import os
import sys
import json
import httpx
import logging
import traceback
import re
import zipfile
import binascii
import hashlib
from io import BytesIO, StringIO
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple, Set
from dataclasses import dataclass
from dotenv import load_dotenv
import xml.etree.ElementTree as ET
import chardet
from bs4 import BeautifulSoup
import markdownify

# === ë„¤ì´ë²„/êµ¬ê¸€ ê²€ìƒ‰ ê´€ë ¨ ì„í¬íŠ¸ ===
from pydantic import BaseModel, Field, ValidationError
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# === SWOT ë¶„ì„ ê´€ë ¨ ì„í¬íŠ¸ ===
from colorama import Fore, Style, init

# === MCP ê´€ë ¨ ì„í¬íŠ¸ ===
from mcp.server.fastmcp import FastMCP, Context
from mcp.types import Tool

# === ì´ˆê¸°í™” ì„¤ì • ===
# .env íŒŒì¼ ë¡œë“œ (íŒŒì¼ì´ ì—†ì–´ë„ ì˜¤ë¥˜ ë°œìƒ ì•ˆ í•¨)
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# í„°ë¯¸ë„ ìƒ‰ìƒ ì§€ì› ì´ˆê¸°í™” (SWOT ë¶„ì„ìš©)
init()

# --- API í‚¤ ë° ì„¤ì • ---
# DART API ì„¤ì •
API_KEY = os.getenv("DART_API_KEY")
BASE_URL = "https://opendart.fss.or.kr/api"

# ë„¤ì´ë²„ API ì„¤ì •
NAVER_API_BASE_URL = "https://openapi.naver.com/v1/search/"
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")
NAVER_HEADERS = {}

if NAVER_CLIENT_ID and NAVER_CLIENT_SECRET:
    NAVER_HEADERS = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
    }
else:
    logger.warning("NAVER_CLIENT_ID ë˜ëŠ” NAVER_CLIENT_SECRET í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# êµ¬ê¸€ API ì„¤ì •
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
GOOGLE_CSE_ID = os.getenv('GOOGLE_CSE_ID')

# MCP ì„œë²„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
mcp = FastMCP("CompanyAnalysis")
logger.info("Company Analysis MCP Server ì´ˆê¸°í™” ì¤‘...")

# ===== SWOT ë¶„ì„ ê´€ë ¨ ì½”ë“œ =====

@dataclass
class SWOTThoughtData:
    """ê¸°ì—… SWOT ë¶„ì„ì„ ìœ„í•œ í–¥ìƒëœ ì‚¬ê³  ë°ì´í„° êµ¬ì¡°."""
    thought: str
    thoughtNumber: int
    totalThoughts: int
    nextThoughtNeeded: bool
    analysisStage: str  # 'S', 'W', 'O', 'T', 'synthesis', 'recommendation', 'planning'
    companyName: Optional[str] = None
    jobPosition: Optional[str] = None
    industry: Optional[str] = None
    isRevision: Optional[bool] = None
    revisesThought: Optional[int] = None
    branchFromThought: Optional[int] = None
    branchId: Optional[str] = None
    needsMoreThoughts: Optional[bool] = None
    dataSource: Optional[str] = None  # ì •ë³´ ì¶œì²˜
    languagePreference: Optional[str] = "ko"  # ì–¸ì–´ ì„¤ì • (ko: í•œêµ­ì–´, en: ì˜ì–´) 

class EnhancedSWOTServer:
    """í–¥ìƒëœ ê¸°ì—… SWOT ë¶„ì„ì„ ìœ„í•œ ì„œë²„ êµ¬í˜„."""

    def __init__(self):
        self.thought_history: List[SWOTThoughtData] = []
        self.branches: Dict[str, List[SWOTThoughtData]] = {}
        
        # SWOT ë¶„ì„ ë‹¨ê³„ ì •ì˜
        self.stages = {
            'planning': 'ğŸ“ ê³„íš ìˆ˜ë¦½',
            'S': 'ğŸ’ª ê°•ì (Strengths)',
            'W': 'ğŸ” ì•½ì (Weaknesses)',
            'O': 'ğŸš€ ê¸°íšŒ(Opportunities)', 
            'T': 'âš ï¸ ìœ„í˜‘(Threats)',
            'synthesis': 'ğŸ”„ ì¢…í•© ë¶„ì„',
            'recommendation': 'âœ… ì§€ì› ì „ëµ'
        }
        
        # ì˜ì–´ ë‹¨ê³„ëª… ì •ì˜
        self.stages_en = {
            'planning': 'ğŸ“ Planning',
            'S': 'ğŸ’ª Strengths',
            'W': 'ğŸ” Weaknesses',
            'O': 'ğŸš€ Opportunities', 
            'T': 'âš ï¸ Threats',
            'synthesis': 'ğŸ”„ Synthesis',
            'recommendation': 'âœ… Strategy'
        }
        
        # ë‹¨ê³„ë³„ ì¶”ì²œ ì§ˆë¬¸ê³¼ ì²´í¬ë¦¬ìŠ¤íŠ¸
        self.stage_prompts = {
            'planning': [
                "ë¶„ì„í•  ê¸°ì—…ê³¼ ì§ë¬´ë¥¼ ëª…í™•íˆ ì •ì˜í–ˆëŠ”ê°€?",
                "ì–´ë–¤ ì •ë³´ ì¶œì²˜ë¥¼ í™œìš©í•  ê³„íšì¸ê°€?",
                "ì™¸ë¶€ ë„êµ¬(ê¸°ì—… í™ˆí˜ì´ì§€, ë‰´ìŠ¤ ê²€ìƒ‰, ì¬ë¬´ì •ë³´ ì‚¬ì´íŠ¸ ë“±)ë¥¼ ì–´ë–»ê²Œ í™œìš©í•  ê²ƒì¸ê°€?",
                "ë¶„ì„ì˜ ì£¼ìš” ëª©ì ì€ ë¬´ì—‡ì¸ê°€? (ë©´ì ‘ ì¤€ë¹„, ìì†Œì„œ ì‘ì„± ë“±)",
                "ë¶„ì„ ì¼ì •ê³¼ ë‹¨ê³„ë¥¼ ì–´ë–»ê²Œ ê³„íší•  ê²ƒì¸ê°€?"
            ],
            'S': [
                "ê¸°ì—…ì˜ ì‹œì¥ ì ìœ ìœ¨ê³¼ ë¸Œëœë“œ ê°€ì¹˜ëŠ” ì–´ë– í•œê°€?",
                "í•µì‹¬ ì œí’ˆ/ì„œë¹„ìŠ¤ì˜ ê²½ìŸ ìš°ìœ„ ìš”ì†ŒëŠ” ë¬´ì—‡ì¸ê°€?",
                "ê¸°ìˆ ë ¥, íŠ¹í—ˆ, ì§€ì ì¬ì‚°ê¶Œ í˜„í™©ì€ ì–´ë– í•œê°€?",
                "ê¸°ì—… ë¬¸í™”ì™€ ì¸ì¬ ê´€ë¦¬ì˜ ê°•ì ì€ ë¬´ì—‡ì¸ê°€?",
                "ì¬ë¬´ ìƒíƒœì™€ íˆ¬ì ëŠ¥ë ¥ì€ ì–´ë– í•œê°€?"
            ],
            'W': [
                "ê²½ìŸì‚¬ ëŒ€ë¹„ ë¶€ì¡±í•œ ë¶€ë¶„ì€ ë¬´ì—‡ì¸ê°€?",
                "ë‚´ë¶€ í”„ë¡œì„¸ìŠ¤ë‚˜ ì‹œìŠ¤í…œì˜ ë¹„íš¨ìœ¨ì„±ì´ ìˆëŠ”ê°€?",
                "ì¸ë ¥, ê¸°ìˆ , ìì›ì˜ ì œí•œì ì€ ë¬´ì—‡ì¸ê°€?",
                "ì‹œì¥ ëŒ€ì‘ ì†ë„ë‚˜ í˜ì‹  ëŠ¥ë ¥ì— ì•½ì ì´ ìˆëŠ”ê°€?",
                "ë¶€ì •ì  í‰íŒì´ë‚˜ ê³¼ê±° ì‹¤íŒ¨ ì‚¬ë¡€ê°€ ìˆëŠ”ê°€?"
            ],
            'O': [
                "ì‹œì¥ ì„±ì¥ ê°€ëŠ¥ì„±ê³¼ ìƒˆë¡œìš´ íŠ¸ë Œë“œëŠ” ë¬´ì—‡ì¸ê°€?",
                "ê¸°ìˆ  ë°œì „ìœ¼ë¡œ ì¸í•œ ìƒˆë¡œìš´ ê¸°íšŒëŠ” ë¬´ì—‡ì¸ê°€?",
                "ê²½ìŸì‚¬ì˜ ì•½ì ì„ í™œìš©í•  ìˆ˜ ìˆëŠ” ì˜ì—­ì€ ë¬´ì—‡ì¸ê°€?",
                "ê·œì œ ë³€í™”, ì‚¬íšŒì  ë³€í™”ë¡œ ì¸í•œ ê¸°íšŒëŠ” ë¬´ì—‡ì¸ê°€?",
                "ì‹ ê·œ ì‹œì¥ ì§„ì¶œ ê°€ëŠ¥ì„±ì€ ìˆëŠ”ê°€?"
            ],
            'T': [
                "ì£¼ìš” ê²½ìŸì‚¬ì™€ ê²½ìŸ ì‹¬í™” ìš”ì¸ì€ ë¬´ì—‡ì¸ê°€?",
                "ì‹œì¥ ë³€í™”ì™€ ì†Œë¹„ì ë‹ˆì¦ˆ ë³€í™”ëŠ” ì–´ë– í•œê°€?",
                "ì‹ ê¸°ìˆ ì´ë‚˜ ëŒ€ì²´ì¬ë¡œ ì¸í•œ ìœ„í˜‘ì€ ë¬´ì—‡ì¸ê°€?",
                "ê·œì œ, ë²•ì  ìœ„í˜‘ ìš”ì†ŒëŠ” ë¬´ì—‡ì¸ê°€?",
                "ê²½ì œ, ì •ì¹˜ì  ë¦¬ìŠ¤í¬ ìš”ì¸ì€ ë¬´ì—‡ì¸ê°€?"
            ],
            'synthesis': [
                "SWOT ìš”ì†Œë“¤ ê°„ì˜ ìƒí˜¸ì‘ìš©ì€ ì–´ë– í•œê°€?",
                "ê°€ì¥ ì¤‘ìš”í•œ í†µì°°ì€ ë¬´ì—‡ì¸ê°€?",
                "ê¸°ì—…ì˜ ì „ëµì  ë°©í–¥ì„±ì€ ë¬´ì—‡ì¸ê°€?",
                "SO, WO, ST, WT ì „ëµì„ ì–´ë–»ê²Œ ìˆ˜ë¦½í•  ìˆ˜ ìˆëŠ”ê°€?"
            ],
            'recommendation': [
                "ìì‹ ì˜ ê°•ì ê³¼ ê¸°ì—… í•„ìš”ì„±ì„ ì–´ë–»ê²Œ ì—°ê²°í•  ìˆ˜ ìˆëŠ”ê°€?",
                "ê¸°ì—… ë¬¸í™” ì í•©ì„±ì„ ì–´ë–»ê²Œ ì œì‹œí•  ê²ƒì¸ê°€?",
                "ë©´ì ‘ê³¼ ìê¸°ì†Œê°œì„œì—ì„œ ì–´ë–¤ ì°¨ë³„í™” ì „ëµì„ ì‚¬ìš©í•  ê²ƒì¸ê°€?",
                "ì…ì‚¬ í›„ ì–´ë–»ê²Œ ê¸°ì—¬í•  ìˆ˜ ìˆëŠ”ê°€?",
                "ì§€ì› ê³¼ì •ì—ì„œ í™œìš©í•  í•µì‹¬ í¬ì¸íŠ¸ëŠ” ë¬´ì—‡ì¸ê°€?"
            ]
        }

        # ë‹¨ê³„ë³„ í…œí”Œë¦¿ (ì–‘ì‹)
        self.stage_templates = {
            'planning': "ë¶„ì„ ëª©í‘œ: {company_name} {position}\nê³„íš ë‹¨ê³„: {plan_stage}\ní™œìš© ìë£Œ: {resources}\nì£¼ìš” ì´ˆì : {focus}",
            'S': "S{num}: {strength_title}\nê·¼ê±°: {evidence}\nì¤‘ìš”ë„: {importance}",
            'W': "W{num}: {weakness_title}\nê·¼ê±°: {evidence}\nëŒ€ì‘ ë°©ì•ˆ: {counter_measure}",
            'O': "O{num}: {opportunity_title}\nê·¼ê±°: {evidence}\ní™œìš© ë°©ì•ˆ: {leverage}",
            'T': "T{num}: {threat_title}\nê·¼ê±°: {evidence}\nëŒ€ë¹„ ë°©ì•ˆ: {mitigation}",
            'synthesis': "SO ì „ëµ: {so_strategy}\nWO ì „ëµ: {wo_strategy}\nST ì „ëµ: {st_strategy}\nWT ì „ëµ: {wt_strategy}",
            'recommendation': "í•µì‹¬ ì°¨ë³„ì : {key_differentiator}\nìì†Œì„œ í¬ì¸íŠ¸: {resume_points}\në©´ì ‘ ë‹µë³€ ì „ëµ: {interview_strategy}\nì…ì‚¬ í›„ ê¸°ì—¬: {contribution}"
        }

    def visual_length(self, s: str) -> int:
        """ë¬¸ìì—´ì˜ ì‹œê°ì  ê¸¸ì´ ê³„ì‚° (í•œê¸€/CJK ë¬¸ìëŠ” ë„ˆë¹„ê°€ 2)"""
        length = 0
        for c in s:
            # í•œê¸€, í•œì, ì¼ë³¸ì–´, ì´ëª¨ì§€ ë“± ë„“ì€ ë¬¸ì ì²˜ë¦¬
            if (0x1100 <= ord(c) <= 0x11FF or  # í•œê¸€ ìëª¨
                0x3130 <= ord(c) <= 0x318F or  # í•œê¸€ í˜¸í™˜ ìëª¨
                0xAC00 <= ord(c) <= 0xD7AF or  # í•œê¸€ ìŒì ˆ
                0x4E00 <= ord(c) <= 0x9FFF or  # CJK í†µí•© í•œì
                0x3000 <= ord(c) <= 0x303F or  # CJK ê¸°í˜¸ ë° ë¬¸ì¥ ë¶€í˜¸
                0xFF00 <= ord(c) <= 0xFFEF or  # ì „ê° ë¬¸ì
                0x1F300 <= ord(c) <= 0x1F64F):  # ì´ëª¨ì§€
                length += 2
            else:
                length += 1
        return length

    def validate_thought_data(self, input_data: Any) -> SWOTThoughtData:
        """ì…ë ¥ ì‚¬ê³  ë°ì´í„° ê²€ì¦."""
        if not isinstance(input_data, dict):
            raise ValueError("ì…ë ¥ì€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤")

        # í•„ìˆ˜ í•„ë“œ í™•ì¸
        required_fields = ["thought", "thoughtNumber", "totalThoughts", "nextThoughtNeeded", "analysisStage"]
        for field in required_fields:
            if field not in input_data:
                raise ValueError(f"í•„ìˆ˜ í•„ë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {field}")

        # íƒ€ì… ê²€ì¦
        if not isinstance(input_data.get("thought"), str):
            raise ValueError("Invalid thought: must be a string")
        if not isinstance(input_data.get("thoughtNumber"), int):
            raise ValueError("Invalid thoughtNumber: must be an integer")
        if not isinstance(input_data.get("totalThoughts"), int):
            raise ValueError("Invalid totalThoughts: must be an integer")
        if not isinstance(input_data.get("nextThoughtNeeded"), bool):
            raise ValueError("Invalid nextThoughtNeeded: must be a boolean")
        if not isinstance(input_data.get("analysisStage"), str):
            raise ValueError("Invalid analysisStage: must be a string")
        
        # ë¶„ì„ ë‹¨ê³„ ê²€ì¦
        if input_data.get("analysisStage") not in self.stages:
            raise ValueError(f"Invalid analysisStage: must be one of {', '.join(self.stages.keys())}")

        # SWOTThoughtData ê°ì²´ ìƒì„±
        return SWOTThoughtData(
            thought=input_data.get("thought"),
            thoughtNumber=input_data.get("thoughtNumber"),
            totalThoughts=input_data.get("totalThoughts"),
            nextThoughtNeeded=input_data.get("nextThoughtNeeded"),
            analysisStage=input_data.get("analysisStage"),
            companyName=input_data.get("companyName"),
            jobPosition=input_data.get("jobPosition"),
            industry=input_data.get("industry"),
            isRevision=input_data.get("isRevision"),
            revisesThought=input_data.get("revisesThought"),
            branchFromThought=input_data.get("branchFromThought"),
            branchId=input_data.get("branchId"),
            needsMoreThoughts=input_data.get("needsMoreThoughts"),
            dataSource=input_data.get("dataSource"),
            languagePreference=input_data.get("languagePreference", "ko")
        )

    def format_thought(self, thought_data: SWOTThoughtData) -> str:
        """SWOT ë¶„ì„ ë‹¨ê³„ì— ë§ê²Œ ì‚¬ê³  í¬ë§·íŒ…."""
        # ì–¸ì–´ ì„¤ì •ì— ë”°ë¥¸ ë‹¨ê³„ ì •ë³´ ì„ íƒ
        is_korean = thought_data.languagePreference != "en"
        stages = self.stages if is_korean else self.stages_en
        
        # ê³ ì • ë°•ìŠ¤ ë„ˆë¹„ ì„¤ì • (ì¼ì •í•œ í¬ê¸° ìœ ì§€ë¥¼ ìœ„í•´ - í„°ë¯¸ë„ ë„ˆë¹„ì— ë§ì¶¤)
        FIXED_BOX_WIDTH = 70  # ë” ì‘ì€ ê°’ìœ¼ë¡œ ì¡°ì •í•˜ì—¬ ëŒ€ë¶€ë¶„ì˜ í„°ë¯¸ë„ì— ë§ë„ë¡ í•¨
        
        # ë¶„ì„ ë‹¨ê³„ì— ë”°ë¥¸ ìƒ‰ìƒ ë° ì•„ì´ì½˜
        stage_info = stages.get(thought_data.analysisStage, "")
        
        if thought_data.analysisStage == 'S':
            stage_color = Fore.GREEN
        elif thought_data.analysisStage == 'W':
            stage_color = Fore.RED
        elif thought_data.analysisStage == 'O':
            stage_color = Fore.BLUE
        elif thought_data.analysisStage == 'T':
            stage_color = Fore.YELLOW
        elif thought_data.analysisStage == 'synthesis':
            stage_color = Fore.MAGENTA
        elif thought_data.analysisStage == 'recommendation':
            stage_color = Fore.CYAN
        elif thought_data.analysisStage == 'planning':
            stage_color = Fore.WHITE
        else:
            stage_color = Fore.WHITE
            
        # í—¤ë” êµ¬ì„±
        header_parts = []
        
        # ê¸°ì—…ëª… í‘œì‹œ (ìµœëŒ€ ê¸¸ì´ ì œí•œ)
        if thought_data.companyName:
            company_name = thought_data.companyName
            if len(company_name) > 12:
                company_name = company_name[:10] + ".."
            header_parts.append(f"ğŸ“Š {company_name}")
            
        # ì§ë¬´ í‘œì‹œ (ìµœëŒ€ ê¸¸ì´ ì œí•œ)
        if thought_data.jobPosition:
            job_position = thought_data.jobPosition
            if len(job_position) > 12:
                job_position = job_position[:10] + ".."
            header_parts.append(f"ğŸ‘” {job_position}")
            
        # ë¶„ì„ ë‹¨ê³„ í‘œì‹œ
        header_parts.append(f"{stage_color}{stage_info}{Style.RESET_ALL}")
        
        # ìƒê° ë²ˆí˜¸ í‘œì‹œ
        header_parts.append(f"({thought_data.thoughtNumber}/{thought_data.totalThoughts})")
        
        # ìˆ˜ì •/ë¶„ê¸° ì •ë³´
        context = ""
        if thought_data.isRevision:
            context = f" ({('ìƒê°' if is_korean else 'thought')} {thought_data.revisesThought} {('ìˆ˜ì •' if is_korean else 'revision')})"
        elif thought_data.branchFromThought:
            branch_text = "ìƒê°" if is_korean else "thought"
            from_text = "ì—ì„œ ë¶„ê¸°" if is_korean else "branch from"
            id_text = "ID" # ë™ì¼
            context = f" ({branch_text} {thought_data.branchFromThought}{from_text}, {id_text}: {thought_data.branchId})"
        
        header = " | ".join(header_parts) + context
        
        # í—¤ë” ê¸¸ì´ ê³„ì‚° (ANSI ìƒ‰ìƒ ì½”ë“œ ì œì™¸ ë° í•œê¸€ ê³ ë ¤)
        visible_header_len = self.visual_length(header) - (len(stage_color) + len(Style.RESET_ALL))
        
        # ì •ë³´ ì¶œì²˜ í‘œì‹œ
        source_info = ""
        if thought_data.dataSource:
            source_text = "ì¶œì²˜" if is_korean else "Source"
            source_info_text = f"ğŸ“š {source_text}: {thought_data.dataSource}"
            # ê¸¸ì´ ì œí•œ
            source_visual_len = self.visual_length(source_info_text)
            if source_visual_len > FIXED_BOX_WIDTH - 6:
                # í•œê¸€ ê³ ë ¤í•˜ì—¬ ìë¥´ê¸°
                cut_pos = 0
                current_len = 0
                for i, c in enumerate(source_info_text):
                    char_width = 2 if self.visual_length(c) == 2 else 1
                    if current_len + char_width > FIXED_BOX_WIDTH - 9:
                        break
                    current_len += char_width
                    cut_pos = i + 1
                source_info_text = source_info_text[:cut_pos] + "..."
            
            # ì˜¤ë¥¸ìª½ ì—¬ë°± ê³ ì •ì„ ìœ„í•´ ì •í™•í•œ ê¸¸ì´ ê³„ì‚°
            padding = max(0, FIXED_BOX_WIDTH - 4 - self.visual_length(source_info_text))
            source_info = f"\nâ”‚ {source_info_text}{' ' * padding} â”‚"
        
        # ì¶”ì²œ ì§ˆë¬¸ í‘œì‹œ (í˜„ì¬ ë‹¨ê³„ì— ë§ëŠ” ì²« ë²ˆì§¸ ì§ˆë¬¸ë§Œ)
        prompt_info = ""
        if thought_data.analysisStage in self.stage_prompts and self.stage_prompts[thought_data.analysisStage]:
            prompt_text = "ì¶”ì²œ ì§ˆë¬¸" if is_korean else "Suggested Question"
            question = self.stage_prompts[thought_data.analysisStage][0]  # ì²« ë²ˆì§¸ ì§ˆë¬¸ë§Œ í‘œì‹œ
            prompt_info_text = f"ğŸ’¡ {prompt_text}: {question}"
            
            # ê¸¸ì´ ì œí•œ
            prompt_visual_len = self.visual_length(prompt_info_text)
            if prompt_visual_len > FIXED_BOX_WIDTH - 6:
                # í•œê¸€ ê³ ë ¤í•˜ì—¬ ìë¥´ê¸°
                cut_pos = 0
                current_len = 0
                for i, c in enumerate(prompt_info_text):
                    char_width = 2 if self.visual_length(c) == 2 else 1
                    if current_len + char_width > FIXED_BOX_WIDTH - 9:
                        break
                    current_len += char_width
                    cut_pos = i + 1
                prompt_info_text = prompt_info_text[:cut_pos] + "..."
            
            # ì˜¤ë¥¸ìª½ ì—¬ë°± ê³ ì •ì„ ìœ„í•´ ì •í™•í•œ ê¸¸ì´ ê³„ì‚°
            padding = max(0, FIXED_BOX_WIDTH - 4 - self.visual_length(prompt_info_text))
            prompt_info = f"\nâ”‚ {prompt_info_text}{' ' * padding} â”‚"
        
        # ê³ ì •ëœ í…Œë‘ë¦¬ ê¸¸ì´ ì‚¬ìš©
        border = "â”€" * FIXED_BOX_WIDTH
        
        # ìµœì¢… í¬ë§·íŒ…ëœ ì¶œë ¥ êµ¬ì„±
        formatted_output = f"\nâ”Œ{border}â”\n"
        
        # ì˜¤ë¥¸ìª½ ì—¬ë°± ê³ ì •ì„ ìœ„í•´ ì •í™•í•œ ê¸¸ì´ ê³„ì‚°
        header_padding = max(0, FIXED_BOX_WIDTH - visible_header_len - 2)
        formatted_output += f"â”‚ {header}{' ' * header_padding}â”‚"
        
        if source_info:
            formatted_output += source_info
        if prompt_info:
            formatted_output += prompt_info
            
        formatted_output += f"\nâ”œ{border}â”¤\n"
        
        # ìƒê° ë‚´ìš© í¬ë§·íŒ… - ë„ˆë¬´ ê¸´ ì¤„ì€ ìë¥´ê³  ì—¬ëŸ¬ ì¤„ë¡œ ë‚˜ëˆ„ê¸°
        thought_lines = []
        for line in thought_data.thought.split('\n'):
            # ë¹ˆ ì¤„ì´ë©´ ê·¸ëŒ€ë¡œ ì¶”ê°€
            if not line.strip():
                thought_lines.append("")
                continue
                
            # ì‹¤ì œ í‘œì‹œë˜ëŠ” ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ìë¥´ê¸°
            max_width = FIXED_BOX_WIDTH - 6  # ì—¬ìœ  ê³µê°„ í™•ë³´
            
            while self.visual_length(line) > max_width:
                # ì ì ˆí•œ ìë¥´ê¸° ìœ„ì¹˜ ì°¾ê¸°
                cut_pos = 0
                current_length = 0
                
                for i, c in enumerate(line):
                    char_width = 2 if self.visual_length(c) == 2 else 1
                    if current_length + char_width > max_width:
                        break
                    current_length += char_width
                    cut_pos = i + 1
                
                # ì ì ˆí•œ ë¶„í•  ì§€ì  ì°¾ê¸° (ê³µë°± ê¸°ì¤€)
                space_pos = line[:cut_pos].rfind(' ')
                if space_pos > max_width // 3:  # ì¶©ë¶„íˆ ì•ìª½ì— ê³µë°±ì´ ìˆìœ¼ë©´ ê·¸ ìœ„ì¹˜ì—ì„œ ìë¦„
                    cut_pos = space_pos + 1
                    
                thought_lines.append(line[:cut_pos].rstrip())
                line = line[cut_pos:].lstrip()
            
            if line:  # ë‚¨ì€ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì¶”ê°€
                thought_lines.append(line)
        
        # ë‚´ìš© ì¶œë ¥ (í•œê¸€ ë° íŠ¹ìˆ˜ë¬¸ì ê³ ë ¤)
        for line in thought_lines:
            # ì‹œê°ì  ê¸¸ì´ ê³„ì‚°
            visual_len = self.visual_length(line)
            # ì •í™•í•œ íŒ¨ë”© ê³„ì‚°
            padding = max(0, FIXED_BOX_WIDTH - 4 - visual_len)
            formatted_output += f"â”‚ {line}{' ' * padding} â”‚\n"
            
        formatted_output += f"â””{border}â”˜"
        
        return formatted_output

    def get_next_stage_hint(self, current_stage: str, is_korean: bool = True) -> Tuple[str, str]:
        """ë‹¤ìŒ ë¶„ì„ ë‹¨ê³„ ë° íŒíŠ¸ ì œê³µ."""
        stages_order = ['planning', 'S', 'W', 'O', 'T', 'synthesis', 'recommendation']
        
        try:
            current_index = stages_order.index(current_stage)
            if current_index < len(stages_order) - 1:
                next_stage = stages_order[current_index + 1]
                
                hints = {
                    'planning': "ê°•ì (S) ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”. ê¸°ì—…ì˜ í•µì‹¬ ê²½ìŸë ¥, ì‹œì¥ ìœ„ì¹˜, ê¸°ìˆ ë ¥ ë“±ì„ ì¡°ì‚¬í•˜ì„¸ìš”." if is_korean else 
                               "Begin Strengths analysis. Research the company's core competencies, market position, and technical capabilities.",
                    'S': "ì•½ì (W) ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”. ê¸°ì—…ì˜ ë¶€ì¡±í•œ ì , ê°œì„  í•„ìš” ì˜ì—­ì„ íŒŒì•…í•˜ì„¸ìš”." if is_korean else 
                         "Begin Weaknesses analysis. Identify areas where the company lags behind competitors or needs improvement.",
                    'W': "ê¸°íšŒ(O) ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”. ê¸°ì—…ì´ í™œìš©í•  ìˆ˜ ìˆëŠ” ì‹œì¥ íŠ¸ë Œë“œì™€ ì™¸ë¶€ ìš”ì¸ì„ ì¡°ì‚¬í•˜ì„¸ìš”." if is_korean else 
                         "Begin Opportunities analysis. Research market trends and external factors the company could leverage.",
                    'O': "ìœ„í˜‘(T) ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”. ê¸°ì—…ì˜ ì„±ì¥ì„ ì €í•´í•  ìˆ˜ ìˆëŠ” ìœ„í—˜ ìš”ì†Œë¥¼ íŒŒì•…í•˜ì„¸ìš”." if is_korean else 
                         "Begin Threats analysis. Identify potential risks that could hinder the company's growth.",
                    'T': "ì¢…í•© ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”. SWOT ìš”ì†Œ ê°„ ìƒí˜¸ì‘ìš©ì„ ë¶„ì„í•˜ê³  SO, WO, ST, WT ì „ëµì„ ë„ì¶œí•˜ì„¸ìš”." if is_korean else 
                         "Begin Synthesis. Analyze interactions between SWOT elements and derive SO, WO, ST, WT strategies.",
                    'synthesis': "ì§€ì› ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”. ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìê¸°ì†Œê°œì„œ í¬ì¸íŠ¸ì™€ ë©´ì ‘ ë‹µë³€ ì „ëµì„ ì¤€ë¹„í•˜ì„¸ìš”." if is_korean else 
                                "Develop your application strategy. Prepare points for your resume and interview based on the analysis."
                }
                
                return next_stage, hints.get(current_stage, "ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•˜ì„¸ìš”." if is_korean else "Proceed to the next stage.")
            else:
                return "", "ëª¨ë“  ë¶„ì„ ë‹¨ê³„ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!" if is_korean else "All analysis stages completed!"
        except ValueError:
            return "planning", "ê³„íš ìˆ˜ë¦½ë¶€í„° ì‹œì‘í•˜ì„¸ìš”." if is_korean else "Start with the Planning stage."
    
    def process_thought(self, input_data: Any) -> Dict[str, Any]:
        """í–¥ìƒëœ SWOT ì‚¬ê³  ì²˜ë¦¬ ë° ì‘ë‹µ ë°˜í™˜."""
        try:
            validated_input = self.validate_thought_data(input_data)
            is_korean = validated_input.languagePreference != "en"

            # ì´ ìƒê° ìˆ˜ê°€ í˜„ì¬ ìƒê° ë²ˆí˜¸ë³´ë‹¤ ì‘ìœ¼ë©´ ì—…ë°ì´íŠ¸
            if validated_input.thoughtNumber > validated_input.totalThoughts:
                validated_input.totalThoughts = validated_input.thoughtNumber

            # ìƒê° ê¸°ë¡ì— ì¶”ê°€
            self.thought_history.append(validated_input)

            # ë¶„ê¸° ì²˜ë¦¬
            if validated_input.branchFromThought and validated_input.branchId:
                if validated_input.branchId not in self.branches:
                    self.branches[validated_input.branchId] = []
                self.branches[validated_input.branchId].append(validated_input)

            # ì¶œë ¥ í¬ë§·íŒ…
            formatted_thought = self.format_thought(validated_input)
            print(formatted_thought, file=sys.stderr)
            
            # ë‹¤ìŒ ë‹¨ê³„ íŒíŠ¸ ê³„ì‚°
            next_stage, next_hint = "", ""
            if validated_input.nextThoughtNeeded:
                next_stage, next_hint = self.get_next_stage_hint(validated_input.analysisStage, is_korean)
            
            # í˜„ì¬ ë‹¨ê³„ ì¶”ì²œ ì§ˆë¬¸ (ìµœëŒ€ 3ê°œ)
            current_prompts = []
            if validated_input.analysisStage in self.stage_prompts:
                current_prompts = self.stage_prompts[validated_input.analysisStage][:3]  # ìµœëŒ€ 3ê°œ ì§ˆë¬¸
            
            # í˜„ì¬ ë‹¨ê³„ í…œí”Œë¦¿
            stage_template = self.stage_templates.get(validated_input.analysisStage, "")

            return {
                "thoughtNumber": validated_input.thoughtNumber,
                "totalThoughts": validated_input.totalThoughts,
                "nextThoughtNeeded": validated_input.nextThoughtNeeded,
                "analysisStage": validated_input.analysisStage,
                "branches": list(self.branches.keys()),
                "thoughtHistoryLength": len(self.thought_history),
                "nextStage": next_stage,
                "nextStageHint": next_hint,
                "currentStagePrompts": current_prompts,
                "stageTemplate": stage_template
            }
        except Exception as e:
            error_msg = f"ì‚¬ê³  ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}" if is_korean else f"Error processing thought: {e}"
            print(f"{Fore.RED}{error_msg}{Style.RESET_ALL}", file=sys.stderr)
            raise

# SWOT ë¶„ì„ ì„œë²„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
swot_server = EnhancedSWOTServer()
# logger.info("SWOT ë¶„ì„ ì„œë²„ ì´ˆê¸°í™” ì™„ë£Œ")

# SWOT ë¶„ì„ ì„¤ëª… ë¬¸ì„œ
ENHANCED_SWOT_DESCRIPTION = """
ì·¨ì—… ì¤€ë¹„ë¥¼ ìœ„í•œ ê¸°ì—… SWOT ë¶„ì„ ë„êµ¬ì…ë‹ˆë‹¤.
ì´ ë„êµ¬ëŠ” ì²´ê³„ì ì´ê³  ë‹¨ê³„ì ì¸ ë°©ë²•ìœ¼ë¡œ ê¸°ì—…ì„ ë¶„ì„í•˜ê³  ì§€ì› ì „ëµì„ ìˆ˜ë¦½í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.

ì–¸ì œ ì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ë‚˜ìš”:
- ì·¨ì—… ì§€ì› ëŒ€ìƒ ê¸°ì—…ì— ëŒ€í•´ ì² ì €íˆ ë¶„ì„í•˜ê³  ì‹¶ì„ ë•Œ
- ë©´ì ‘ ì¤€ë¹„ë¥¼ ìœ„í•´ ê¸°ì—…ì— ëŒ€í•œ í†µì°°ë ¥ì„ ì–»ê³  ì‹¶ì„ ë•Œ
- ìê¸°ì†Œê°œì„œë‚˜ ë©´ì ‘ì—ì„œ ê¸°ì—… ë§ì¶¤í˜• ë‹µë³€ì„ ì¤€ë¹„í•  ë•Œ
- ì—¬ëŸ¬ ê¸°ì—… ì¤‘ ì–´ë””ì— ì§€ì›í• ì§€ ê²°ì •í•˜ê¸° ìœ„í•œ ë¹„êµ ë¶„ì„ì´ í•„ìš”í•  ë•Œ
- íŠ¹ì • ì‚°ì—…ì´ë‚˜ ì§ë¬´ì— ëŒ€í•œ ì „ëµì  ì´í•´ê°€ í•„ìš”í•  ë•Œ

ì£¼ìš” íŠ¹ì§•:
- ê¸°ì—…ì˜ ê°•ì (S), ì•½ì (W), ê¸°íšŒ(O), ìœ„í˜‘(T)ì„ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„
- ë¶„ì„ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì  ê²°ë¡  ë„ì¶œ
- ê¸°ì—… ë§ì¶¤í˜• ì§€ì› ì „ëµ ìˆ˜ë¦½
- ë‹¤ì–‘í•œ ì •ë³´ ì¶œì²˜ë¥¼ í™œìš©í•œ ë¶„ì„
- í•„ìš”ì‹œ ì´ì „ ë¶„ì„ ë‹¨ê³„ ìˆ˜ì • ê°€ëŠ¥
- ì§ë¬´ë³„ íŠ¹í™”ëœ ë¶„ì„ ì§€ì›

ë§¤ê°œë³€ìˆ˜ ì„¤ëª…:
- thought: í˜„ì¬ ë¶„ì„ ë‹¨ê³„ì—ì„œì˜ ìƒê°ì´ë‚˜ í†µì°°
- thoughtNumber: í˜„ì¬ ìƒê° ë²ˆí˜¸ (ìµœì†Œê°’: 1)
- totalThoughts: ì˜ˆìƒë˜ëŠ” ì´ ìƒê° ìˆ˜ (ìµœì†Œê°’: 1)
- nextThoughtNeeded: ì¶”ê°€ ìƒê°ì´ í•„ìš”í•œì§€ ì—¬ë¶€
- analysisStage: í˜„ì¬ ë¶„ì„ ë‹¨ê³„ ('S', 'W', 'O', 'T', 'synthesis', 'recommendation')
- companyName: ë¶„ì„ ëŒ€ìƒ ê¸°ì—…ëª…
- jobPosition: ì§€ì› ì§ë¬´
- industry: ì‚°ì—… ë¶„ì•¼
- isRevision: ì´ì „ ìƒê°ì„ ìˆ˜ì •í•˜ëŠ”ì§€ ì—¬ë¶€
- revisesThought: ìˆ˜ì • ëŒ€ìƒ ìƒê° ë²ˆí˜¸
- branchFromThought: ë¶„ê¸° ì‹œì‘ì  ìƒê° ë²ˆí˜¸
- branchId: ë¶„ê¸° ì‹ë³„ì
- needsMoreThoughts: ì¶”ê°€ ìƒê°ì´ í•„ìš”í•œì§€ ì—¬ë¶€
- dataSource: ì •ë³´ ì¶œì²˜
- languagePreference: ì–¸ì–´ ì„¤ì • (ko: í•œêµ­ì–´, en: ì˜ì–´)
"""

# ===== DART API ê´€ë ¨ ì½”ë“œ =====

# íŠ¹ì • ëŸ°íƒ€ì„ ì—ëŸ¬ ë¡œê·¸ í•„í„°ë§ ì„¤ì •
class IgnoreRuntimeErrorFilter(logging.Filter):
    def filter(self, record):
        # ë¡œê·¸ ë©”ì‹œì§€ì— íŠ¹ì • ë¬¸ìì—´ì´ í¬í•¨ë˜ì–´ ìˆê³ , ì˜ˆì™¸ ì •ë³´ê°€ RuntimeError íƒ€ì…ì¸ì§€ í™•ì¸
        if 'RuntimeError: Attempted to exit cancel scope in a different task' in record.getMessage() \
           and record.exc_info and isinstance(record.exc_info[1], RuntimeError):
            return False # ì´ ë¡œê·¸ëŠ” í•„í„°ë§ (ì¶œë ¥ ì•ˆ í•¨)
        return True # ë‹¤ë¥¸ ë¡œê·¸ëŠ” í†µê³¼

# ê¸°ë³¸ ë¡œê±° ê°€ì ¸ì˜¤ê¸° ë° í•„í„° ì¶”ê°€
logger.addFilter(IgnoreRuntimeErrorFilter())

async def get_financial_statement_xbrl(rcept_no: str, reprt_code: str) -> str:
    """
    ì¬ë¬´ì œí‘œ ì›ë³¸íŒŒì¼(XBRL)ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ XBRL í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜

    Args:
        rcept_no: ê³µì‹œ ì ‘ìˆ˜ë²ˆí˜¸(14ìë¦¬)
        reprt_code: ë³´ê³ ì„œ ì½”ë“œ (11011: ì‚¬ì—…ë³´ê³ ì„œ, 11012: ë°˜ê¸°ë³´ê³ ì„œ, 11013: 1ë¶„ê¸°ë³´ê³ ì„œ, 11014: 3ë¶„ê¸°ë³´ê³ ì„œ)

    Returns:
        ì¶”ì¶œëœ XBRL í…ìŠ¤íŠ¸ ë‚´ìš©, ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ë©”ì‹œì§€ ë¬¸ìì—´
    """
    url = f"{BASE_URL}/fnlttXbrl.xml?crtfc_key={API_KEY}&rcept_no={rcept_no}&reprt_code={reprt_code}"

    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(url)

            if response.status_code != 200:
                return f"API ìš”ì²­ ì‹¤íŒ¨: HTTP ìƒíƒœ ì½”ë“œ {response.status_code}"
            
            # ì‘ë‹µ ë°ì´í„° ê¸°ë³¸ ì •ë³´ ë¡œê¹…
            content_type = response.headers.get('content-type', 'ì•Œ ìˆ˜ ì—†ìŒ')
            content_length = len(response.content)
            content_md5 = hashlib.md5(response.content).hexdigest()
            
            logger.info(f"DART API ì‘ë‹µ ì •ë³´: URL={url}, ìƒíƒœì½”ë“œ={response.status_code}, Content-Type={content_type}, í¬ê¸°={content_length}ë°”ì´íŠ¸, MD5={content_md5}")

            try:
                with zipfile.ZipFile(BytesIO(response.content)) as zip_file:
                    xbrl_content = ""
                    for file_name in zip_file.namelist():
                        if file_name.lower().endswith('.xbrl'):
                            with zip_file.open(file_name) as xbrl_file:
                                # XBRL íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ì½ê¸° (UTF-8 ì‹œë„, ì‹¤íŒ¨ ì‹œ EUC-KR)
                                try:
                                    xbrl_content = xbrl_file.read().decode('utf-8')
                                except UnicodeDecodeError:
                                    try:
                                        xbrl_file.seek(0)
                                        xbrl_content = xbrl_file.read().decode('euc-kr')
                                    except UnicodeDecodeError:
                                        xbrl_content = "<ì¸ì½”ë”© ì˜¤ë¥˜: XBRL ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤>"
                            break 
                    
                    if not xbrl_content:
                        return "ZIP íŒŒì¼ ë‚´ì—ì„œ XBRL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    
                    return xbrl_content

            except zipfile.BadZipFile:
                # ì‘ë‹µì´ ZIP íŒŒì¼ í˜•ì‹ì´ ì•„ë‹ ê²½ìš° (DART API ì˜¤ë¥˜ ë©”ì‹œì§€ ë“±)
                logger.error(f"ìœ íš¨í•˜ì§€ ì•Šì€ ZIP íŒŒì¼: URL={url}, Content-Type={content_type}, í¬ê¸°={content_length}ë°”ì´íŠ¸")
                
                # íŒŒì¼ ì‹œì‘ ë¶€ë¶„(ì²˜ìŒ 50~100ë°”ì´íŠ¸) 16ì§„ìˆ˜ë¡œ ë¤í”„í•˜ì—¬ ë¡œê¹…
                content_head = response.content[:100]
                hex_dump = binascii.hexlify(content_head).decode('utf-8')
                hex_formatted = ' '.join(hex_dump[i:i+2] for i in range(0, len(hex_dump), 2))
                logger.error(f"ìœ íš¨í•˜ì§€ ì•Šì€ ZIP íŒŒì¼ í—¤ë” ë¤í”„(100ë°”ì´íŠ¸): {hex_formatted}")
                
                try:
                    # ì—¬ëŸ¬ ì¸ì½”ë”©ìœ¼ë¡œ í•´ì„ ì‹œë„í•˜ê³  ë‚´ìš© ë¡œê¹…
                    encodings_to_try = ['utf-8', 'euc-kr', 'cp949', 'latin-1']
                    decoded_contents = {}
                    
                    for encoding in encodings_to_try:
                        try:
                            content_preview = response.content[:1000].decode(encoding)
                            content_preview = content_preview.replace('\n', ' ')[:200]  # ì¤„ë°”ê¿ˆ ì œê±°, 200ìë¡œ ì œí•œ
                            decoded_contents[encoding] = content_preview
                            logger.info(f"{encoding} ì¸ì½”ë”©ìœ¼ë¡œ í•´ì„í•œ ë‚´ìš©(ì¼ë¶€): {content_preview}")
                        except UnicodeDecodeError:
                            logger.info(f"{encoding} ì¸ì½”ë”©ìœ¼ë¡œ í•´ì„ ì‹¤íŒ¨")
                    
                    # XML íŒŒì‹± ì‹œë„
                    try:
                        error_content = response.content.decode('utf-8')
                        try:
                            root = ET.fromstring(error_content)
                            status = root.findtext('status')
                            message = root.findtext('message')
                            if status and message:
                                logger.info(f"API ì‘ë‹µì„ XMLë¡œ íŒŒì‹± ì„±ê³µ: status={status}, message={message}")
                                return f"DART API ì˜¤ë¥˜: {status} - {message}"
                            else:
                                return f"ìœ íš¨í•˜ì§€ ì•Šì€ ZIP íŒŒì¼ì´ë©°, ì˜¤ë¥˜ ë©”ì‹œì§€ íŒŒì‹± ì‹¤íŒ¨: {error_content[:200]}"
                        except ET.ParseError as xml_err:
                            logger.error(f"XML íŒŒì‹± ì˜¤ë¥˜: {xml_err}")
                            return f"ìœ íš¨í•˜ì§€ ì•Šì€ ZIP íŒŒì¼ì´ë©°, XML íŒŒì‹± ë¶ˆê°€: {error_content[:200]}"
                    except UnicodeDecodeError as decode_err:
                        logger.error(f"ì‘ë‹µ ë‚´ìš© ë””ì½”ë”© ì˜¤ë¥˜: {decode_err}")
                        # ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ ë°”ì´ë„ˆë¦¬ ë°ì´í„° ì¶”ê°€ ì •ë³´ ë¡œê¹…
                        try:
                            # íŒŒì¼ ì‹œê·¸ë‹ˆì²˜ í™•ì¸ (ì²˜ìŒ 4~8ë°”ì´íŠ¸)
                            file_sig_hex = binascii.hexlify(response.content[:8]).decode('utf-8')
                            logger.info(f"íŒŒì¼ ì‹œê·¸ë‹ˆì²˜(HEX): {file_sig_hex}")
                            
                            # ì¼ë°˜ì ì¸ íŒŒì¼ í˜•ì‹ë“¤ì˜ ì‹œê·¸ë‹ˆì²˜ì™€ ë¹„êµ
                            known_signatures = {
                                "504b0304": "ZIP íŒŒì¼(ì •ìƒ)",
                                "3c3f786d": "XML ë¬¸ì„œ",
                                "7b227374": "JSON ë¬¸ì„œ",
                                "1f8b0800": "GZIP ì••ì¶•íŒŒì¼",
                                "ffd8ffe0": "JPEG ì´ë¯¸ì§€",
                                "89504e47": "PNG ì´ë¯¸ì§€",
                                "25504446": "PDF ë¬¸ì„œ"
                            }
                            
                            for sig, desc in known_signatures.items():
                                if file_sig_hex.startswith(sig):
                                    logger.info(f"íŒŒì¼ í˜•ì‹ ì¸ì‹: {desc}")
                            
                            return "ë‹¤ìš´ë¡œë“œí•œ íŒŒì¼ì´ ìœ íš¨í•œ ZIP íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤ (ë°”ì´ë„ˆë¦¬ ë‚´ìš© ë””ë²„ê¹… ë¡œê·¸ í™•ì¸)."
                        except Exception as bin_err:
                            logger.error(f"ë°”ì´ë„ˆë¦¬ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {bin_err}")
                            return "ë‹¤ìš´ë¡œë“œí•œ íŒŒì¼ì´ ìœ íš¨í•œ ZIP íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤ (ë‚´ìš© í™•ì¸ ë¶ˆê°€)."
                
                except Exception as decode_err:
                    logger.error(f"ì‘ë‹µ ë‚´ìš© ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {decode_err}")
                    return f"ë‹¤ìš´ë¡œë“œí•œ íŒŒì¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(decode_err)}"
            
            except Exception as e:
                logger.error(f"ZIP íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                return f"ZIP íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    except httpx.RequestError as e:
        logger.error(f"API ìš”ì²­ ì¤‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
        return f"API ìš”ì²­ ì¤‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    except Exception as e:
        logger.error(f"XBRL ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return f"XBRL ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

# ìƒìˆ˜ ì •ì˜
# API ì„¤ì •
API_KEY = os.getenv("DART_API_KEY")
BASE_URL = "https://opendart.fss.or.kr/api"

# ë³´ê³ ì„œ ì½”ë“œ
REPORT_CODE = {
    "ì‚¬ì—…ë³´ê³ ì„œ": "11011",
    "ë°˜ê¸°ë³´ê³ ì„œ": "11012",
    "1ë¶„ê¸°ë³´ê³ ì„œ": "11013",
    "3ë¶„ê¸°ë³´ê³ ì„œ": "11014"
}

# ì¬ë¬´ìƒíƒœí‘œ í•­ëª© ë¦¬ìŠ¤íŠ¸ - í™•ì¥
BALANCE_SHEET_ITEMS = [
    "ìœ ë™ìì‚°", "ë¹„ìœ ë™ìì‚°", "ìì‚°ì´ê³„", 
    "ìœ ë™ë¶€ì±„", "ë¹„ìœ ë™ë¶€ì±„", "ë¶€ì±„ì´ê³„", 
    "ìë³¸ê¸ˆ", "ìë³¸ì‰ì—¬ê¸ˆ", "ì´ìµì‰ì—¬ê¸ˆ", "ê¸°íƒ€ìë³¸í•­ëª©", "ìë³¸ì´ê³„"
]

# í˜„ê¸ˆíë¦„í‘œ í•­ëª© ë¦¬ìŠ¤íŠ¸
CASH_FLOW_ITEMS = ["ì˜ì—…í™œë™ í˜„ê¸ˆíë¦„", "íˆ¬ìí™œë™ í˜„ê¸ˆíë¦„", "ì¬ë¬´í™œë™ í˜„ê¸ˆíë¦„"]

# ë³´ê³ ì„œ ìœ í˜•ë³„ contextRef íŒ¨í„´ ì •ì˜
REPORT_PATTERNS = {
    "ì—°ê°„": "FY",
    "3ë¶„ê¸°": "TQQ",  # ì†ìµê³„ì‚°ì„œëŠ” TQQ
    "ë°˜ê¸°": "HYA",
    "1ë¶„ê¸°": "FQA"
}

# í˜„ê¸ˆíë¦„í‘œìš© íŠ¹ë³„ íŒ¨í„´
CASH_FLOW_PATTERNS = {
    "ì—°ê°„": "FY",
    "3ë¶„ê¸°": "TQA",  # í˜„ê¸ˆíë¦„í‘œëŠ” TQA
    "ë°˜ê¸°": "HYA",
    "1ë¶„ê¸°": "FQA"
}

# ì¬ë¬´ìƒíƒœí‘œìš© íŠ¹ë³„ íŒ¨í„´
BALANCE_SHEET_PATTERNS = {
    "ì—°ê°„": "FY",
    "3ë¶„ê¸°": "TQA",  # ì¬ë¬´ìƒíƒœí‘œë„ TQA
    "ë°˜ê¸°": "HYA",
    "1ë¶„ê¸°": "FQA"
}

# ë°ì´í„° ë¬´íš¨/ì˜¤ë¥˜ ìƒíƒœ í‘œì‹œì
INVALID_VALUE_INDICATORS = {"N/A", "XBRL íŒŒì‹± ì˜¤ë¥˜", "ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜"}

# ì¬ë¬´ì œí‘œ ìœ í˜• ì •ì˜
STATEMENT_TYPES = {
    "ì¬ë¬´ìƒíƒœí‘œ": "BS",
    "ì†ìµê³„ì‚°ì„œ": "IS", 
    "í˜„ê¸ˆíë¦„í‘œ": "CF"
}

# ì„¸ë¶€ í•­ëª© íƒœê·¸ ì •ì˜
DETAILED_TAGS = {
    "ì¬ë¬´ìƒíƒœí‘œ": {
        "ìœ ë™ìì‚°": ["ifrs-full:CurrentAssets"],
        "ë¹„ìœ ë™ìì‚°": ["ifrs-full:NoncurrentAssets"],
        "ìì‚°ì´ê³„": ["ifrs-full:Assets"],
        "ìœ ë™ë¶€ì±„": ["ifrs-full:CurrentLiabilities"],
        "ë¹„ìœ ë™ë¶€ì±„": ["ifrs-full:NoncurrentLiabilities"],
        "ë¶€ì±„ì´ê³„": ["ifrs-full:Liabilities"],
        "ìë³¸ê¸ˆ": ["ifrs-full:IssuedCapital"],
        "ìë³¸ì‰ì—¬ê¸ˆ": ["ifrs-full:SharePremium"],
        "ì´ìµì‰ì—¬ê¸ˆ": ["ifrs-full:RetainedEarnings"],
        "ê¸°íƒ€ìë³¸í•­ëª©": ["dart:ElementsOfOtherStockholdersEquity"],
        "ìë³¸ì´ê³„": ["ifrs-full:Equity"]
    },
    "ì†ìµê³„ì‚°ì„œ": {
        "ë§¤ì¶œì•¡": ["ifrs-full:Revenue"],
        "ë§¤ì¶œì›ê°€": ["ifrs-full:CostOfSales"],
        "ë§¤ì¶œì´ì´ìµ": ["ifrs-full:GrossProfit"],
        "íŒë§¤ë¹„ì™€ê´€ë¦¬ë¹„": ["dart:TotalSellingGeneralAdministrativeExpenses"],
        "ì˜ì—…ì´ìµ": ["dart:OperatingIncomeLoss"],
        "ê¸ˆìœµìˆ˜ìµ": ["ifrs-full:FinanceIncome"],
        "ê¸ˆìœµë¹„ìš©": ["ifrs-full:FinanceCosts"],
        "ë²•ì¸ì„¸ë¹„ìš©ì°¨ê°ì „ìˆœì´ìµ": ["ifrs-full:ProfitLossBeforeTax"],
        "ë²•ì¸ì„¸ë¹„ìš©": ["ifrs-full:IncomeTaxExpenseContinuingOperations"],
        "ë‹¹ê¸°ìˆœì´ìµ": ["ifrs-full:ProfitLoss"],
        "ê¸°ë³¸ì£¼ë‹¹ì´ìµ": ["ifrs-full:BasicEarningsLossPerShare"]
    },
    "í˜„ê¸ˆíë¦„í‘œ": {
        "ì˜ì—…í™œë™ í˜„ê¸ˆíë¦„": ["ifrs-full:CashFlowsFromUsedInOperatingActivities"],
        "ì˜ì—…ì—ì„œ ì°½ì¶œëœ í˜„ê¸ˆ": ["ifrs-full:CashFlowsFromUsedInOperations"],
        "ì´ììˆ˜ì·¨": ["ifrs-full:InterestReceivedClassifiedAsOperatingActivities"],
        "ì´ìì§€ê¸‰": ["ifrs-full:InterestPaidClassifiedAsOperatingActivities"],
        "ë°°ë‹¹ê¸ˆìˆ˜ì·¨": ["ifrs-full:DividendsReceivedClassifiedAsOperatingActivities"],
        "ë²•ì¸ì„¸ë‚©ë¶€": ["ifrs-full:IncomeTaxesPaidRefundClassifiedAsOperatingActivities"],
        "íˆ¬ìí™œë™ í˜„ê¸ˆíë¦„": ["ifrs-full:CashFlowsFromUsedInInvestingActivities"],
        "ìœ í˜•ìì‚°ì˜ ì·¨ë“": ["ifrs-full:PurchaseOfPropertyPlantAndEquipmentClassifiedAsInvestingActivities"],
        "ë¬´í˜•ìì‚°ì˜ ì·¨ë“": ["ifrs-full:PurchaseOfIntangibleAssetsClassifiedAsInvestingActivities"],
        "ìœ í˜•ìì‚°ì˜ ì²˜ë¶„": ["ifrs-full:ProceedsFromSalesOfPropertyPlantAndEquipmentClassifiedAsInvestingActivities"],
        "ì¬ë¬´í™œë™ í˜„ê¸ˆíë¦„": ["ifrs-full:CashFlowsFromUsedInFinancingActivities"],
        "ë°°ë‹¹ê¸ˆì§€ê¸‰": ["ifrs-full:DividendsPaidClassifiedAsFinancingActivities"],
        "í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°ì˜ìˆœì¦ê°€": ["ifrs-full:IncreaseDecreaseInCashAndCashEquivalents"],
        "ê¸°ì´ˆí˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°": ["dart:CashAndCashEquivalentsAtBeginningOfPeriodCf"],
        "ê¸°ë§í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°": ["dart:CashAndCashEquivalentsAtEndOfPeriodCf"]
    }
}

chat_guideline = "\n* ì œê³µëœ ê³µì‹œì •ë³´ë“¤ì€ ë¶„ê¸°, ë°˜ê¸°, ì—°ê°„ì´ ì„ì—¬ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \nì‚¬ìš©ìê°€ íŠ¹ë³„íˆ ì—°ê°„ì´ë‚˜ ë°˜ê¸°ë°ì´í„°ë§Œì„ ì›í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ë©´, ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ì ë‹¹íˆ ê°€ê³µí•˜ì—¬ ë¶„ê¸°ë³„ë¡œ ì‚¬ìš©ìì—ê²Œ ì œê³µí•˜ì„¸ìš”."

# === DART API Helper í•¨ìˆ˜ ===

async def get_corp_code_by_name(corp_name: str) -> Tuple[str, str]:
    """
    íšŒì‚¬ëª…ìœ¼ë¡œ íšŒì‚¬ì˜ ê³ ìœ ë²ˆí˜¸ë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        corp_name: ê²€ìƒ‰í•  íšŒì‚¬ëª…
        
    Returns:
        (ê³ ìœ ë²ˆí˜¸, ê¸°ì—…ì´ë¦„) íŠœí”Œ, ì°¾ì§€ ëª»í•œ ê²½ìš° ("", "")
    """
    url = f"{BASE_URL}/corpCode.xml?crtfc_key={API_KEY}"
    
    logger.info(f"íšŒì‚¬ ì½”ë“œ ê²€ìƒ‰ ì‹œì‘: íšŒì‚¬ëª…='{corp_name}', URL={url}")
    
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                logger.info(f"DART API ìš”ì²­ ì‹œì‘: corpCode.xml API í˜¸ì¶œ ì¤‘")
                response = await client.get(url)
                
                logger.info(f"DART API ìš”ì²­ ì™„ë£Œ: ì‘ë‹µ ìƒíƒœì½”ë“œ={response}")
                
                # ì‘ë‹µ ë°ì´í„° ê¸°ë³¸ ì •ë³´ ë¡œê¹…
                content_type = response.headers.get('content-type', 'ì•Œ ìˆ˜ ì—†ìŒ')
                content_length = len(response.content)
                content_md5 = hashlib.md5(response.content).hexdigest()
                
                logger.info(f"corpCode API ì‘ë‹µ ì •ë³´: ìƒíƒœì½”ë“œ={response.status_code}, Content-Type={content_type}, í¬ê¸°={content_length}ë°”ì´íŠ¸, MD5={content_md5}")
                
                if response.status_code != 200:
                    logger.error(f"corpCode API ìš”ì²­ ì‹¤íŒ¨: HTTP ìƒíƒœ ì½”ë“œ {response.status_code}")
                    return ("", f"API ìš”ì²­ ì‹¤íŒ¨: HTTP ìƒíƒœ ì½”ë“œ {response.status_code}")
                
                try:
                    logger.info("ZIP íŒŒì¼ ì••ì¶• í•´ì œ ì‹œë„")
                    with zipfile.ZipFile(BytesIO(response.content)) as zip_file:
                        try:
                            file_list = zip_file.namelist()
                            logger.info(f"ZIP íŒŒì¼ ë‚´ íŒŒì¼ ëª©ë¡: {file_list}")
                            
                            if 'CORPCODE.xml' not in file_list:
                                logger.error("ZIP íŒŒì¼ ë‚´ì— CORPCODE.xmlì´ ì—†ìŠµë‹ˆë‹¤")
                                return ("", "ZIP íŒŒì¼ ë‚´ì— CORPCODE.xmlì´ ì—†ìŠµë‹ˆë‹¤")
                            
                            logger.info("CORPCODE.xml íŒŒì¼ ì—´ê¸° ì‹œë„")
                            with zip_file.open('CORPCODE.xml') as xml_file:
                                try:
                                    logger.info("XML íŒŒì‹± ì‹œë„")
                                    tree = ET.parse(xml_file)
                                    root = tree.getroot()
                                    
                                    # XML ê¸°ë³¸ ì •ë³´ ë¡œê¹…
                                    company_count = len(root.findall('.//list'))
                                    logger.info(f"ì „ì²´ íšŒì‚¬ ëª©ë¡ ìˆ˜: {company_count}ê°œ")
                                    
                                    # ê²€ìƒ‰ì–´ë¥¼ í¬í•¨í•˜ëŠ” ëª¨ë“  íšŒì‚¬ ì°¾ê¸°
                                    logger.info(f"'{corp_name}' ê²€ìƒ‰ì–´ë¡œ íšŒì‚¬ ê²€ìƒ‰ ì‹œì‘")
                                    matches = []
                                    match_count = 0
                                    for company in root.findall('.//list'):
                                        name = company.find('corp_name').text
                                        stock_code = company.find('stock_code').text
                                        
                                        # stock_codeê°€ ë¹„ì–´ìˆê±°ë‚˜ ê³µë°±ë§Œ ìˆëŠ” ê²½ìš° ê±´ë„ˆë›°ê¸°
                                        if not stock_code or stock_code.strip() == "":
                                            continue
                                            
                                        if name and corp_name in name:
                                            match_count += 1
                                            # ì¼ì¹˜ë„ ì ìˆ˜ ê³„ì‚° (ë‚®ì„ìˆ˜ë¡ ë” ì •í™•íˆ ì¼ì¹˜)
                                            score = 0
                                            if name != corp_name:
                                                score += abs(len(name) - len(corp_name))
                                                if not name.startswith(corp_name):
                                                    score += 10
                                            
                                            code = company.find('corp_code').text
                                            matches.append((name, code, score))
                                            
                                            if match_count <= 5:  # ì²˜ìŒ 5ê°œ íšŒì‚¬ë§Œ ë¡œê¹…
                                                logger.info(f"ê²€ìƒ‰ ê²°ê³¼ í›„ë³´: ì´ë¦„='{name}', ì½”ë“œ={code}, ì£¼ì‹ì½”ë“œ={stock_code}, ì¼ì¹˜ë„ì ìˆ˜={score}")
                                    
                                    # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ë¡œê¹…
                                    logger.info(f"ì´ {match_count}ê°œ íšŒì‚¬ê°€ '{corp_name}' ê²€ìƒ‰ì–´ì™€ ì¼ì¹˜")
                                    
                                    # ì¼ì¹˜í•˜ëŠ” íšŒì‚¬ê°€ ì—†ëŠ” ê²½ìš°
                                    if not matches:
                                        logger.warning(f"'{corp_name}' íšŒì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                        return ("", f"'{corp_name}' íšŒì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                    
                                    # ì¼ì¹˜ë„ ì ìˆ˜ê°€ ê°€ì¥ ë‚®ì€ (ê°€ì¥ ì¼ì¹˜í•˜ëŠ”) íšŒì‚¬ ë°˜í™˜
                                    matches.sort(key=lambda x: x[2])
                                    matched_name = matches[0][0]
                                    matched_code = matches[0][1]
                                    logger.info(f"ê°€ì¥ ì¼ì¹˜í•˜ëŠ” íšŒì‚¬ ì„ íƒ: ì´ë¦„='{matched_name}', ì½”ë“œ={matched_code}")
                                    return (matched_code, matched_name)
                                except ET.ParseError as e:
                                    logger.error(f"XML íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                                    return ("", f"XML íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                        except Exception as e:
                            logger.error(f"ZIP íŒŒì¼ ë‚´ë¶€ íŒŒì¼ ì ‘ê·¼ ì˜¤ë¥˜: {str(e)}")
                            return ("", f"ZIP íŒŒì¼ ë‚´ë¶€ íŒŒì¼ ì ‘ê·¼ ì˜¤ë¥˜: {str(e)}")
                except zipfile.BadZipFile:
                    logger.error(f"ìœ íš¨í•˜ì§€ ì•Šì€ ZIP íŒŒì¼: URL={url}, Content-Type={content_type}, í¬ê¸°={content_length}ë°”ì´íŠ¸")
                    
                    # íŒŒì¼ ì‹œì‘ ë¶€ë¶„(ì²˜ìŒ 50~100ë°”ì´íŠ¸) 16ì§„ìˆ˜ë¡œ ë¤í”„í•˜ì—¬ ë¡œê¹…
                    content_head = response.content[:100]
                    hex_dump = binascii.hexlify(content_head).decode('utf-8')
                    hex_formatted = ' '.join(hex_dump[i:i+2] for i in range(0, len(hex_dump), 2))
                    logger.error(f"ìœ íš¨í•˜ì§€ ì•Šì€ ZIP íŒŒì¼ í—¤ë” ë¤í”„(100ë°”ì´íŠ¸): {hex_formatted}")
                    
                    return ("", "ë‹¤ìš´ë¡œë“œí•œ íŒŒì¼ì´ ìœ íš¨í•œ ZIP íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤.")
                except Exception as e:
                    logger.error(f"ZIP íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    return ("", f"ZIP íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            except httpx.RequestError as e:
                logger.error(f"API ìš”ì²­ ì¤‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                return ("", f"API ìš”ì²­ ì¤‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    except Exception as e:
        logger.error(f"íšŒì‚¬ ì½”ë“œ ì¡°íšŒ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}, ìŠ¤íƒíŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        return ("", f"íšŒì‚¬ ì½”ë“œ ì¡°íšŒ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


async def get_disclosure_list(corp_code: str, start_date: str, end_date: str) -> Tuple[List[Dict[str, Any]], Optional[str]]:
    """
    ê¸°ì—…ì˜ ì •ê¸°ê³µì‹œ ëª©ë¡ì„ ì¡°íšŒí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        corp_code: íšŒì‚¬ ê³ ìœ ë²ˆí˜¸(8ìë¦¬)
        start_date: ì‹œì‘ì¼(YYYYMMDD)
        end_date: ì¢…ë£Œì¼(YYYYMMDD)
    
    Returns:
        (ê³µì‹œ ëª©ë¡ ë¦¬ìŠ¤íŠ¸, ì˜¤ë¥˜ ë©”ì‹œì§€) íŠœí”Œ. ì„±ê³µ ì‹œ (ëª©ë¡, None), ì‹¤íŒ¨ ì‹œ (ë¹ˆ ë¦¬ìŠ¤íŠ¸, ì˜¤ë¥˜ ë©”ì‹œì§€)
    """
    # ì •ê¸°ê³µì‹œ(A) ìœ í˜•ë§Œ ì¡°íšŒ
    url = f"{BASE_URL}/list.json?crtfc_key={API_KEY}&corp_code={corp_code}&bgn_de={start_date}&end_de={end_date}&pblntf_ty=A&page_count=100"
    
    logger.info(f"ê³µì‹œ ëª©ë¡ ì¡°íšŒ ì‹œì‘: íšŒì‚¬ì½”ë“œ={corp_code}, ì‹œì‘ì¼={start_date}, ì¢…ë£Œì¼={end_date}")
    logger.info(f"ê³µì‹œ ëª©ë¡ API URL: {url}")
    
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                logger.info("DART ê³µì‹œ ëª©ë¡ API ìš”ì²­ ì‹œì‘")
                response = await client.get(url)
                
                # ì‘ë‹µ ë°ì´í„° ê¸°ë³¸ ì •ë³´ ë¡œê¹…
                content_type = response.headers.get('content-type', 'ì•Œ ìˆ˜ ì—†ìŒ')
                content_length = len(response.content)
                
                logger.info(f"ê³µì‹œ ëª©ë¡ API ì‘ë‹µ ì •ë³´: ìƒíƒœì½”ë“œ={response.status_code}, Content-Type={content_type}, í¬ê¸°={content_length}ë°”ì´íŠ¸")
                
                if response.status_code != 200:
                    logger.error(f"ê³µì‹œ ëª©ë¡ API ìš”ì²­ ì‹¤íŒ¨: HTTP ìƒíƒœ ì½”ë“œ {response.status_code}")
                    return [], f"API ìš”ì²­ ì‹¤íŒ¨: HTTP ìƒíƒœ ì½”ë“œ {response.status_code}"
                
                try:
                    logger.info("JSON ì‘ë‹µ íŒŒì‹± ì‹œë„")
                    result = response.json()
                    
                    status = result.get('status')
                    msg = result.get('message', 'ë©”ì‹œì§€ ì—†ìŒ')
                    
                    if status != '000':
                        logger.error(f"DART API ì˜¤ë¥˜ ì‘ë‹µ: status={status}, message={msg}")
                        return [], f"DART API ì˜¤ë¥˜: {status} - {msg}"
                    
                    # ì •ìƒ ì‘ë‹µ ì²˜ë¦¬
                    disclosure_list = result.get('list', [])
                    disclosure_count = len(disclosure_list)
                    
                    if disclosure_count > 0:
                        logger.info(f"ê³µì‹œ ëª©ë¡ ì¡°íšŒ ì„±ê³µ: {disclosure_count}ê°œ ê³µì‹œ ë°œê²¬")
                        # ì²« 5ê°œ ê³µì‹œë§Œ ë¡œê¹…
                        for i, disclosure in enumerate(disclosure_list[:5]):
                            report_nm = disclosure.get('report_nm', 'ì œëª© ì—†ìŒ')
                            rcept_dt = disclosure.get('rcept_dt', 'ë‚ ì§œ ì—†ìŒ')
                            rcept_no = disclosure.get('rcept_no', 'ë²ˆí˜¸ ì—†ìŒ')
                            logger.info(f"ê³µì‹œ {i+1}: ì œëª©='{report_nm}', ì ‘ìˆ˜ì¼={rcept_dt}, ì ‘ìˆ˜ë²ˆí˜¸={rcept_no}")
                    else:
                        logger.warning(f"ê³µì‹œ ëª©ë¡ì´ ë¹„ì–´ ìˆìŒ: íšŒì‚¬ì½”ë“œ={corp_code}, ê¸°ê°„={start_date}~{end_date}")
                    
                    return disclosure_list, None
                    
                except ValueError as e:
                    logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                    
                    # ì‘ë‹µ ë‚´ìš© ì¼ë¶€ ë¡œê¹… (JSONì´ ì•„ë‹ ê²½ìš°)
                    try:
                        content_preview = response.content[:500].decode('utf-8')
                        logger.error(f"JSONì´ ì•„ë‹Œ ì‘ë‹µ ë‚´ìš©(ì¼ë¶€): {content_preview}")
                    except UnicodeDecodeError:
                        logger.error("ì‘ë‹µ ë‚´ìš©ì„ UTF-8ë¡œ ë””ì½”ë”©í•  ìˆ˜ ì—†ìŒ (ë°”ì´ë„ˆë¦¬ ë°ì´í„°)")
                        
                    return [], f"ì‘ë‹µ JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}"
                except Exception as e:
                    logger.error(f"ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    return [], f"ì‘ë‹µ JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}"
                    
            except httpx.RequestError as e:
                logger.error(f"ê³µì‹œ ëª©ë¡ API ìš”ì²­ ì¤‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {str(e)}")
                return [], f"API ìš”ì²­ ì¤‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    except Exception as e:
        logger.error(f"ê³µì‹œ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}, ìŠ¤íƒíŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        return [], f"ê³µì‹œ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    logger.error("get_disclosure_list í•¨ìˆ˜ê°€ ì˜ˆìƒì¹˜ ëª»í•˜ê²Œ ì¢…ë£Œë¨")
    return [], "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ë¡œ ê³µì‹œ ëª©ë¡ì„ ì¡°íšŒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

# ===== ë„¤ì´ë²„ ê²€ìƒ‰ API ê´€ë ¨ ì½”ë“œ =====

# ë„¤ì´ë²„ API ì„¤ì •
NAVER_API_BASE_URL = "https://openapi.naver.com/v1/search/"
NAVER_CLIENT_ID = os.environ.get("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.environ.get("NAVER_CLIENT_SECRET")
NAVER_HEADERS = {}

if NAVER_CLIENT_ID and NAVER_CLIENT_SECRET:
    NAVER_HEADERS = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
    }
else:
    logger.warning("NAVER_CLIENT_ID ë˜ëŠ” NAVER_CLIENT_SECRET í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# Pydantic ëª¨ë¸ ì •ì˜
class BaseItem(BaseModel):
    title: Optional[str] = None
    link: Optional[str] = None

    class Config:
        extra = "ignore"

class DescriptionItem(BaseItem):
    description: Optional[str] = None

class BlogItem(DescriptionItem):
    bloggername: Optional[str] = None
    bloggerlink: Optional[str] = None
    postdate: Optional[str] = None

class NewsItem(DescriptionItem):
    originallink: Optional[str] = None
    pubDate: Optional[str] = None

class CafeArticleItem(DescriptionItem):
    cafename: Optional[str] = None
    cafeurl: Optional[str] = None

class KinItem(DescriptionItem):
    pass

WebkrItem = DescriptionItem
DocItem = DescriptionItem

class BookItem(BaseItem):
    image: Optional[str] = None
    author: Optional[str] = None
    price: Optional[str] = None
    discount: Optional[str] = None
    publisher: Optional[str] = None
    pubdate: Optional[str] = None
    isbn: Optional[str] = None
    description: Optional[str] = None

class ShopItem(BaseItem):
    image: Optional[str] = None
    lprice: Optional[str] = None
    hprice: Optional[str] = None
    mallName: Optional[str] = None
    productId: Optional[str] = None
    productType: Optional[str] = None
    maker: Optional[str] = None
    brand: Optional[str] = None
    category1: Optional[str] = None
    category2: Optional[str] = None
    category3: Optional[str] = None
    category4: Optional[str] = None

class ImageItem(BaseItem):
    thumbnail: Optional[str] = None
    sizeheight: Optional[str] = None
    sizewidth: Optional[str] = None

class LocalItem(BaseItem):
    category: Optional[str] = None
    description: Optional[str] = None
    telephone: Optional[str] = None
    address: Optional[str] = None
    roadAddress: Optional[str] = None
    mapx: Optional[str] = None
    mapy: Optional[str] = None

class EncycItem(BaseItem):
    thumbnail: Optional[str] = None
    description: Optional[str] = None

# ë‹¨ì¼ ê²°ê³¼ API ëª¨ë¸
class AdultResult(BaseModel): adult: str
class ErrataResult(BaseModel): errata: str

# ê²€ìƒ‰ ê²°ê³¼ ê³µí†µ êµ¬ì¡°
class SearchResultBase(BaseModel):
    lastBuildDate: Optional[str] = None
    total: int = 0
    start: int = 1
    display: int = 10
    items: List[Any] = [] # ê¸°ë³¸ê°’ ë¹ˆ ë¦¬ìŠ¤íŠ¸

# ê° APIë³„ ìµœì¢… ì‘ë‹µ ëª¨ë¸ ì •ì˜
class BlogResult(SearchResultBase): items: List[BlogItem]
class NewsResult(SearchResultBase): items: List[NewsItem]
class CafeArticleResult(SearchResultBase): items: List[CafeArticleItem]
class KinResult(SearchResultBase): items: List[KinItem]
class WebkrResult(SearchResultBase): items: List[WebkrItem]
class DocResult(SearchResultBase): items: List[DocItem]
class BookResult(SearchResultBase): items: List[BookItem]
class ShopResult(SearchResultBase): items: List[ShopItem]
class ImageResult(SearchResultBase): items: List[ImageItem]
class LocalResult(SearchResultBase): items: List[LocalItem]
class EncycResult(SearchResultBase): items: List[EncycItem]

# ì˜¤ë¥˜ ì‘ë‹µ ëª¨ë¸
class ErrorResponse(BaseModel):
    error: str
    details: Optional[str] = None
    status_code: Optional[int] = None

# ë„¤ì´ë²„ API í˜¸ì¶œ ê³µí†µ í•¨ìˆ˜
async def _make_naver_api_call(
    endpoint: str,
    params: Dict[str, Any],
    result_model: BaseModel,
    search_type_name: str # ë™ì  í”„ë¡¬í”„íŠ¸ ìƒì„±ì„ ìœ„í•œ ê²€ìƒ‰ íƒ€ì… ì´ë¦„ ì¶”ê°€
) -> str:
    """
    Calls the Naver search API and parses the result, returning the result in text format.
    """
    if not NAVER_HEADERS:
        logger.error("ë„¤ì´ë²„ API ì¸ì¦ ì •ë³´ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        error_resp = ErrorResponse(error="ì¸ì¦ ì •ë³´ ë¯¸ì„¤ì •", details="NAVER_CLIENT_ID ë˜ëŠ” NAVER_CLIENT_SECRET í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return "ì˜¤ë¥˜ ë°œìƒ:\n" + f"ì˜¤ë¥˜: {error_resp.error}\nì„¸ë¶€ì‚¬í•­: {error_resp.details}"

    url = f"{NAVER_API_BASE_URL}{endpoint}"
    prompt_string = "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:" # ê¸°ë³¸ ì˜¤ë¥˜ í”„ë¡¬í”„íŠ¸

    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            logger.info(f"ë„¤ì´ë²„ API í˜¸ì¶œ ì‹œì‘ - URL: {url}, Params: {params}")
            response = await client.get(url, headers=NAVER_HEADERS, params=params)
            response.raise_for_status() # HTTP ì˜¤ë¥˜ ì‹œ ì˜ˆì™¸ ë°œìƒ

            data = response.json()
            logger.info(f"API ì‘ë‹µ ì„±ê³µ (ìƒíƒœ ì½”ë“œ: {response.status_code})")

            try:
                # Pydantic ëª¨ë¸ë¡œ íŒŒì‹± ë° ìœ íš¨ì„± ê²€ì‚¬
                result = result_model.model_validate(data)
                logger.info(f"ë°ì´í„° íŒŒì‹± ì„±ê³µ (ëª¨ë¸: {result_model.__name__})")

                # ë™ì  Prompt ìƒì„± (SearchResultBase ìƒì† ëª¨ë¸ì¸ ê²½ìš°)
                if isinstance(result, SearchResultBase):
                    start_index = result.start
                    end_index = result.start + len(result.items) - 1
                    prompt_string = f"ë„¤ì´ë²„ {search_type_name} ê²€ìƒ‰ ê²°ê³¼ (ì´ {result.total:,}ê±´ ì¤‘ {start_index}~{end_index}ë²ˆì§¸):"
                    
                    # ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    text_result = f"{prompt_string}\n\n"
                    
                    # ê²°ê³¼ í•­ëª© í˜•ì‹í™”
                    for i, item in enumerate(result.items, 1):
                        text_result += f"### ê²°ê³¼ {i}\n"
                        
                        # ì¼ë°˜ì ì¸ í•­ëª© ì²˜ë¦¬ (ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì— ê³µí†µ)
                        if hasattr(item, 'title'):
                            # HTML íƒœê·¸ ì œê±°
                            title = item.title.replace('<b>', '').replace('</b>', '')
                            text_result += f"ì œëª©(title): {title}\n"
                        
                        if hasattr(item, 'link'):
                            text_result += f"ë§í¬(link): {item.link}\n"
                        
                        if hasattr(item, 'description') and item.description:
                            # HTML íƒœê·¸ ì œê±°
                            desc = item.description.replace('<b>', '').replace('</b>', '')
                            text_result += f"ì„¤ëª…(description): {desc}\n"
                        
                        # ëª¨ë¸ë³„ íŠ¹ìˆ˜ í•„ë“œ ì²˜ë¦¬
                        if isinstance(item, BlogItem):
                            text_result += f"ë¸”ë¡œê±°ëª…(bloggername): {item.bloggername}\n"
                            text_result += f"ë¸”ë¡œê·¸ ë§í¬(bloggerlink): {item.bloggerlink}\n"
                            if item.postdate:
                                text_result += f"ì‘ì„±ì¼(postdate): {item.postdate}\n"
                        
                        elif isinstance(item, NewsItem):
                            if item.originallink:
                                text_result += f"ì›ë³¸ ë§í¬(originallink): {item.originallink}\n"
                            if item.pubDate:
                                text_result += f"ë°œí–‰ì¼(pubDate): {item.pubDate}\n"
                        
                        elif isinstance(item, BookItem) or isinstance(item, ShopItem):
                            if hasattr(item, 'image') and item.image:
                                text_result += f"ì´ë¯¸ì§€(image): {item.image}\n"
                            if hasattr(item, 'author') and item.author:
                                text_result += f"ì €ì(author): {item.author}\n"
                            if hasattr(item, 'price') and item.price:
                                text_result += f"ê°€ê²©(price): {item.price}\n"
                            if hasattr(item, 'discount') and item.discount:
                                text_result += f"í• ì¸ê°€(discount): {item.discount}\n"
                            if hasattr(item, 'publisher') and item.publisher:
                                text_result += f"ì¶œíŒì‚¬(publisher): {item.publisher}\n"
                            if hasattr(item, 'pubdate') and item.pubdate:
                                text_result += f"ì¶œíŒì¼(pubdate): {item.pubdate}\n"
                            if hasattr(item, 'isbn') and item.isbn:
                                text_result += f"ISBN(isbn): {item.isbn}\n"
                                
                        elif isinstance(item, ShopItem):
                            if hasattr(item, 'image') and item.image:
                                text_result += f"ì´ë¯¸ì§€(image): {item.image}\n"
                            if hasattr(item, 'lprice') and item.lprice:
                                text_result += f"ìµœì €ê°€(lprice): {item.lprice}\n"
                            if hasattr(item, 'hprice') and item.hprice:
                                text_result += f"ìµœê³ ê°€(hprice): {item.hprice}\n"
                            if hasattr(item, 'mallName') and item.mallName:
                                text_result += f"ì‡¼í•‘ëª°ëª…(mallName): {item.mallName}\n"
                            if hasattr(item, 'brand') and item.brand:
                                text_result += f"ë¸Œëœë“œ(brand): {item.brand}\n"
                            if hasattr(item, 'maker') and item.maker:
                                text_result += f"ì œì¡°ì‚¬(maker): {item.maker}\n"
                            if hasattr(item, 'category1') and item.category1:
                                text_result += f"ì¹´í…Œê³ ë¦¬1(category1): {item.category1}\n"
                            if hasattr(item, 'category2') and item.category2:
                                text_result += f"ì¹´í…Œê³ ë¦¬2(category2): {item.category2}\n"
                            if hasattr(item, 'category3') and item.category3:
                                text_result += f"ì¹´í…Œê³ ë¦¬3(category3): {item.category3}\n"
                            if hasattr(item, 'category4') and item.category4:
                                text_result += f"ì¹´í…Œê³ ë¦¬4(category4): {item.category4}\n"
                                
                        elif isinstance(item, LocalItem):
                            if item.category:
                                text_result += f"ì¹´í…Œê³ ë¦¬(category): {item.category}\n"
                            if item.telephone:
                                text_result += f"ì „í™”ë²ˆí˜¸(telephone): {item.telephone}\n"
                            if item.address:
                                text_result += f"ì£¼ì†Œ(address): {item.address}\n"
                            if item.roadAddress:
                                text_result += f"ë„ë¡œëª…ì£¼ì†Œ(roadAddress): {item.roadAddress}\n"
                            if item.mapx:
                                text_result += f"ì§€ë„ Xì¢Œí‘œ(mapx): {item.mapx}\n"
                            if item.mapy:
                                text_result += f"ì§€ë„ Yì¢Œí‘œ(mapy): {item.mapy}\n"
                        
                        elif isinstance(item, ImageItem):
                            if item.thumbnail:
                                text_result += f"ì¸ë„¤ì¼(thumbnail): {item.thumbnail}\n"
                            if item.sizeheight:
                                text_result += f"ë†’ì´(sizeheight): {item.sizeheight}\n"
                            if item.sizewidth:
                                text_result += f"ë„ˆë¹„(sizewidth): {item.sizewidth}\n"
                        
                        elif isinstance(item, EncycItem):
                            if item.thumbnail:
                                text_result += f"ì¸ë„¤ì¼(thumbnail): {item.thumbnail}\n"
                                
                        elif isinstance(item, CafeArticleItem):
                            if item.cafename:
                                text_result += f"ì¹´í˜ëª…(cafename): {item.cafename}\n"
                            if item.cafeurl:
                                text_result += f"ì¹´í˜ ë§í¬(cafeurl): {item.cafeurl}\n"
                                
                        text_result += "\n"
                    
                    return text_result
                
                elif isinstance(result, AdultResult):
                    prompt_string = f"ë„¤ì´ë²„ {search_type_name} í™•ì¸ ê²°ê³¼:"
                    if result.adult == 0:
                        return f"{prompt_string} ì¼ë°˜ ê²€ìƒ‰ì–´"
                    else:
                        return f"{prompt_string} ì„±ì¸ ê²€ìƒ‰ì–´"
                
                elif isinstance(result, ErrataResult):
                    print(f"ErrataResult: {result}")
                    prompt_string = f"ë„¤ì´ë²„ {search_type_name} í™•ì¸ ê²°ê³¼:"
                    if result.errata == "":
                        return f"{prompt_string} ì˜¤íƒ€ ì—†ìŒ"
                    else:
                        return f"{prompt_string} {result.errata}"
                
                else: # ì˜ˆìƒì¹˜ ëª»í•œ ê²°ê³¼ íƒ€ì…
                    prompt_string = f"ë„¤ì´ë²„ {search_type_name} ì²˜ë¦¬ ê²°ê³¼:"
                    # ê²°ê³¼ë¥¼ JSON í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ ë³€í™˜
                    result_json = json.dumps(result.model_dump(), ensure_ascii=False)
                    return f"{prompt_string}\n{result_json}"

            except ValidationError as e:
                logger.error(f"Pydantic ìœ íš¨ì„± ê²€ì‚¬ ì˜¤ë¥˜: {e}")
                error_resp = ErrorResponse(error="ì‘ë‹µ ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜", details=str(e))
                return f"{prompt_string}\nì˜¤ë¥˜: {error_resp.error}\nì„¸ë¶€ì‚¬í•­: {error_resp.details}"

    except httpx.HTTPStatusError as e:
        logger.error(f"API HTTP ìƒíƒœ ì˜¤ë¥˜: {e.response.status_code} - {e.response.text}", exc_info=True)
        error_resp = ErrorResponse(
            error=f"API ì˜¤ë¥˜ ({e.response.status_code})",
            details=e.response.text,
            status_code=e.response.status_code
        )
        return f"{prompt_string}\nì˜¤ë¥˜: {error_resp.error}\nì„¸ë¶€ì‚¬í•­: {error_resp.details}"
    except httpx.RequestError as e:
        logger.error(f"ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ì˜¤ë¥˜: {e}", exc_info=True)
        error_resp = ErrorResponse(error="ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜", details=f"ë„¤ì´ë²„ API ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
        return f"{prompt_string}\nì˜¤ë¥˜: {error_resp.error}\nì„¸ë¶€ì‚¬í•­: {error_resp.details}"
    except Exception as e:
        logger.exception(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}") # exc_info=Trueì™€ ë™ì¼
        error_resp = ErrorResponse(error="ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜", details=str(e))
        return f"{prompt_string}\nì˜¤ë¥˜: {error_resp.error}\nì„¸ë¶€ì‚¬í•­: {error_resp.details}"

# í˜ì´ì§€ ê³„ì‚° í•¨ìˆ˜
def calculate_start(page: int, display: int) -> int:
    """Calculates the start value for the API call based on the page number and display count."""
    if page < 1:
        page = 1
    start = (page - 1) * display + 1
    # ë„¤ì´ë²„ APIì˜ start ìµœëŒ€ê°’(1000) ì œí•œ ê³ ë ¤
    return min(start, 1000)

# ===== êµ¬ê¸€ ê²€ìƒ‰ API ê´€ë ¨ ì½”ë“œ =====

# êµ¬ê¸€ API ì„¤ì •
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
GOOGLE_CSE_ID = os.getenv('GOOGLE_CSE_ID')

if not GOOGLE_API_KEY or not GOOGLE_CSE_ID:
    logger.warning("GOOGLE_API_KEY ë˜ëŠ” GOOGLE_CSE_ID í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# êµ¬ê¸€ API ìœ íš¨ì„± í™•ì¸
GOOGLE_SEARCH_AVAILABLE = True
try:
    from googleapiclient.discovery import build
    from googleapiclient.errors import HttpError
except ImportError:
    GOOGLE_SEARCH_AVAILABLE = False
    logger.warning("êµ¬ê¸€ ê²€ìƒ‰ APIë¥¼ ìœ„í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install google-api-python-client'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

# ===== MCP Tool ì •ì˜ =====

# --- DART API ê´€ë ¨ ë„êµ¬ ---
@mcp.tool()
async def search_disclosure(
    company_name: str, 
    start_date: str, 
    end_date: str, 
    ctx: Context,
    requested_items: Optional[List[str]] = None,
) -> str:
    """
    íšŒì‚¬ì˜ ì£¼ìš” ì¬ë¬´ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì œê³µí•˜ëŠ” ë„êµ¬.
    requested_itemsê°€ ì£¼ì–´ì§€ë©´ í•´ë‹¹ í•­ëª© ê´€ë ¨ ë°ì´í„°ê°€ ìˆëŠ” ê³µì‹œë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
    
    Args:
        company_name: íšŒì‚¬ëª… (ì˜ˆ: ì‚¼ì„±ì „ì, ë„¤ì´ë²„ ë“±)
        start_date: ì‹œì‘ì¼ (YYYYMMDD í˜•ì‹, ì˜ˆ: 20250101)
        end_date: ì¢…ë£Œì¼ (YYYYMMDD í˜•ì‹, ì˜ˆ: 20251231)
        ctx: MCP Context ê°ì²´
        requested_items: ì‚¬ìš©ìê°€ ìš”ì²­í•œ ì¬ë¬´ í•­ëª© ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["ë§¤ì¶œì•¡", "ì˜ì—…ì´ìµ"]). Noneì´ë©´ ëª¨ë“  ì£¼ìš” í•­ëª©ì„ ëŒ€ìƒìœ¼ë¡œ í•¨. ì‚¬ìš© ê°€ëŠ¥í•œ í•­ëª©: ë§¤ì¶œì•¡, ì˜ì—…ì´ìµ, ë‹¹ê¸°ìˆœì´ìµ, ì˜ì—…í™œë™ í˜„ê¸ˆíë¦„, íˆ¬ìí™œë™ í˜„ê¸ˆíë¦„, ì¬ë¬´í™œë™ í˜„ê¸ˆíë¦„, ìì‚°ì´ê³„, ë¶€ì±„ì´ê³„, ìë³¸ì´ê³„
        
    Returns:
        ê²€ìƒ‰ëœ ê° ê³µì‹œì˜ ì£¼ìš” ì¬ë¬´ ì •ë³´ ìš”ì•½ í…ìŠ¤íŠ¸ (ìš”ì²­ í•­ëª© ê´€ë ¨ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë§Œ)
    """
    # ê²°ê³¼ ë¬¸ìì—´ ì´ˆê¸°í™”
    result = ""
    
    try:
        # ì§„í–‰ ìƒí™© ì•Œë¦¼
        info_msg = f"{company_name}ì˜"
        if requested_items:
            info_msg += f" {', '.join(requested_items)} ê´€ë ¨"
        info_msg += " ì¬ë¬´ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."
        await ctx.info(info_msg) # await ì¶”ê°€
        
        # end_date ì¡°ì •
        original_end_date = end_date
        adjusted_end_date, was_adjusted = adjust_end_date(end_date)
        
        if was_adjusted:
            await ctx.info(f"ê³µì‹œ ì œì¶œ ê¸°ê°„ì„ ê³ ë ¤í•˜ì—¬ ê²€ìƒ‰ ì¢…ë£Œì¼ì„ {original_end_date}ì—ì„œ {adjusted_end_date}ë¡œ ìë™ ì¡°ì •í–ˆìŠµë‹ˆë‹¤.") # await ì¶”ê°€
            end_date = adjusted_end_date
        
        # íšŒì‚¬ ì½”ë“œ ì¡°íšŒ
        corp_code, matched_name = await get_corp_code_by_name(company_name)
        if not corp_code:
            return f"íšŒì‚¬ ê²€ìƒ‰ ì˜¤ë¥˜: {matched_name}"
        
        await ctx.info(f"{matched_name}(ê³ ìœ ë²ˆí˜¸: {corp_code})ì˜ ê³µì‹œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.") # await ì¶”ê°€
        
        # ê³µì‹œ ëª©ë¡ ì¡°íšŒ
        disclosures, error_msg = await get_disclosure_list(corp_code, start_date, end_date)
        if error_msg:
            return f"ê³µì‹œ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {error_msg}"
            
        if not disclosures:
            date_range_msg = f"{start_date}ë¶€í„° {end_date}ê¹Œì§€"
            if was_adjusted:
                date_range_msg += f" (ì›ë˜ ìš”ì²­: {start_date}~{original_end_date}, ê³µì‹œ ì œì¶œ ê¸°ê°„ ê³ ë ¤í•˜ì—¬ í™•ì¥)"
            return f"{date_range_msg} '{matched_name}'(ê³ ìœ ë²ˆí˜¸: {corp_code})ì˜ ì •ê¸°ê³µì‹œê°€ ì—†ìŠµë‹ˆë‹¤."
        
        await ctx.info(f"{len(disclosures)}ê°œì˜ ì •ê¸°ê³µì‹œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. XBRL ë°ì´í„° ì¡°íšŒ ë° ë¶„ì„ì„ ì‹œë„í•©ë‹ˆë‹¤.") # await ì¶”ê°€

        # ì¶”ì¶œí•  ì¬ë¬´ í•­ëª© ë° ê°€ëŠ¥í•œ íƒœê·¸ ë¦¬ìŠ¤íŠ¸ ì •ì˜
        all_items_and_tags = {
            "ë§¤ì¶œì•¡": ["ifrs-full:Revenue"],
            "ì˜ì—…ì´ìµ": ["dart:OperatingIncomeLoss"],
            "ë‹¹ê¸°ìˆœì´ìµ": ["ifrs-full:ProfitLoss"],
            "ì˜ì—…í™œë™ í˜„ê¸ˆíë¦„": ["ifrs-full:CashFlowsFromUsedInOperatingActivities"],
            "íˆ¬ìí™œë™ í˜„ê¸ˆíë¦„": ["ifrs-full:CashFlowsFromUsedInInvestingActivities"],
            "ì¬ë¬´í™œë™ í˜„ê¸ˆíë¦„": ["ifrs-full:CashFlowsFromUsedInFinancingActivities"],
            "ìì‚°ì´ê³„": ["ifrs-full:Assets"],
            "ë¶€ì±„ì´ê³„": ["ifrs-full:Liabilities"],
            "ìë³¸ì´ê³„": ["ifrs-full:Equity"]
        }

        # ì‚¬ìš©ìê°€ ìš”ì²­í•œ í•­ëª©ë§Œ ì¶”ì¶œí•˜ë„ë¡ êµ¬ì„±
        if requested_items:
            items_to_extract = {item: tags for item, tags in all_items_and_tags.items() if item in requested_items}
            if not items_to_extract:
                 unsupported_items = [item for item in requested_items if item not in all_items_and_tags]
                 return f"ìš”ì²­í•˜ì‹  í•­ëª© ì¤‘ ì§€ì›ë˜ì§€ ì•ŠëŠ” í•­ëª©ì´ ìˆìŠµë‹ˆë‹¤: {', '.join(unsupported_items)}. ì§€ì› í•­ëª©: {', '.join(all_items_and_tags.keys())}"
        else:
            items_to_extract = all_items_and_tags
        
        # ê²°ê³¼ ë¬¸ìì—´ ì´ˆê¸°í™”
        result = f"# {matched_name} ì£¼ìš” ì¬ë¬´ ì •ë³´ ({start_date} ~ {end_date})\n"
        if requested_items: 
            result += f"({', '.join(requested_items)} ê´€ë ¨)\n"
        result += "\n"
        
        # ìµœëŒ€ 5ê°œì˜ ê³µì‹œë§Œ ì²˜ë¦¬ (API í˜¸ì¶œ ì œí•œ ë° ì‹œê°„ ê³ ë ¤)
        disclosure_count = min(5, len(disclosures))
        processed_count = 0
        relevant_reports_found = 0
        api_errors = []
        
        # ê° ê³µì‹œë³„ ì²˜ë¦¬
        for disclosure in disclosures[:disclosure_count]:
            report_name = disclosure.get('report_nm', 'ì œëª© ì—†ìŒ')
            rcept_dt = disclosure.get('rcept_dt', 'ë‚ ì§œ ì—†ìŒ')
            rcept_no = disclosure.get('rcept_no', '')

            # ë³´ê³ ì„œ ì½”ë“œ ê²°ì •
            reprt_code = determine_report_code(report_name)
            if not rcept_no or not reprt_code:
                continue

            # ì§„í–‰ ìƒí™© ë³´ê³ 
            processed_count += 1
            await ctx.report_progress(processed_count, disclosure_count) 
            
            await ctx.info(f"ê³µì‹œ {processed_count}/{disclosure_count} ë¶„ì„ ì¤‘: {report_name} (ì ‘ìˆ˜ë²ˆí˜¸: {rcept_no})") # await ì¶”ê°€
            
            # XBRL ë°ì´í„° ì¡°íšŒ
            try:
                xbrl_text = await get_financial_statement_xbrl(rcept_no, reprt_code)
                
                # XBRL íŒŒì‹± ë° ë°ì´í„° ì¶”ì¶œ
                financial_data = {}
                parse_error = None
                
                if not xbrl_text.startswith(("DART API ì˜¤ë¥˜:", "API ìš”ì²­ ì‹¤íŒ¨:", "ZIP íŒŒì¼", "<ì¸ì½”ë”© ì˜¤ë¥˜:")):
                     try:
                         financial_data = parse_xbrl_financial_data(xbrl_text, items_to_extract)
                     except Exception as e:
                         parse_error = e
                         ctx.warning(f"XBRL íŒŒì‹±/ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({report_name}): {e}")
                         financial_data = {key: "ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ" for key in items_to_extract}
                elif xbrl_text.startswith("DART API ì˜¤ë¥˜: 013"):
                    financial_data = {key: "ë°ì´í„° ì—†ìŒ(API 013)" for key in items_to_extract}
                else:
                    error_summary = xbrl_text.split('\n')[0][:100]
                    financial_data = {key: f"ì˜¤ë¥˜({error_summary})" for key in items_to_extract}
                    api_errors.append(f"{report_name}: {error_summary}")
                    await ctx.warning(f"XBRL ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜ ({report_name}): {error_summary}") # ctx.warningì—ë„ await ì¶”ê°€ (ë§Œì•½ ë¹„ë™ê¸°ë¼ë©´)

                # ìš”ì²­ëœ í•­ëª© ê´€ë ¨ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
                is_relevant = True
                if requested_items:
                    is_relevant = any(
                        item in financial_data and 
                        financial_data[item] not in INVALID_VALUE_INDICATORS and 
                        not financial_data[item].startswith("ì˜¤ë¥˜(") and
                        not financial_data[item].startswith("ë¶„ì„ ì¤‘")
                        for item in requested_items
                    )

                # ê´€ë ¨ ë°ì´í„°ê°€ ìˆëŠ” ê³µì‹œë§Œ ê²°ê³¼ì— ì¶”ê°€
                if is_relevant:
                    relevant_reports_found += 1
                    result += f"## {report_name} ({rcept_dt})\n"
                    result += f"ì ‘ìˆ˜ë²ˆí˜¸: {rcept_no}\n\n"
                    
                    if financial_data:
                        for item, value in financial_data.items():
                             result += f"- {item}: {value}\n"
                    elif parse_error:
                         result += f"- XBRL ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {parse_error}\n"
                    else:
                         result += "- ì£¼ìš” ì¬ë¬´ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n"
                    
                    result += "\n" + "-" * 50 + "\n\n"
                else:
                     await ctx.info(f"[{report_name}] ê±´ë„ˆëœ€: ìš”ì²­í•˜ì‹  í•­ëª©({', '.join(requested_items) if requested_items else 'ì „ì²´'}) ê´€ë ¨ ìœ íš¨ ë°ì´í„° ì—†ìŒ.") # await ì¶”ê°€
            except Exception as e:
                await ctx.error(f"ê³µì‹œ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ ({report_name}): {e}") # ctx.errorì—ë„ await ì¶”ê°€ (ë§Œì•½ ë¹„ë™ê¸°ë¼ë©´)
                api_errors.append(f"{report_name}: {str(e)}")
                traceback.print_exc()

        # ìµœì¢… ê²°ê³¼ ë©”ì‹œì§€ ì¶”ê°€
        if api_errors:
            result += "\n## ì²˜ë¦¬ ì¤‘ ë°œìƒí•œ ì˜¤ë¥˜\n"
            for error in api_errors:
                result += f"- {error}\n"
            result += "\n"
            
        if relevant_reports_found == 0 and processed_count > 0:
             no_data_reason = "ìš”ì²­í•˜ì‹  í•­ëª© ê´€ë ¨ ìœ íš¨í•œ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆê±°ë‚˜" if requested_items else "ì£¼ìš” ì¬ë¬´ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆê±°ë‚˜"
             result += f"â€» ì²˜ë¦¬ëœ ê³µì‹œì—ì„œ {no_data_reason}, ë°ì´í„°ê°€ ì œê³µë˜ì§€ ì•ŠëŠ” ë³´ê³ ì„œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
        elif processed_count == 0 and disclosures:
             result += "ì¡°íšŒëœ ì •ê¸°ê³µì‹œê°€ ìˆìœ¼ë‚˜, XBRL ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ” ë³´ê³ ì„œ ìœ í˜•(ì‚¬ì—…/ë°˜ê¸°/ë¶„ê¸°)ì´ ì•„ë‹ˆê±°ë‚˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n"
             
        if len(disclosures) > disclosure_count:
            result += f"â€» ì´ {len(disclosures)}ê°œì˜ ì •ê¸°ê³µì‹œ ì¤‘ ìµœì‹  {disclosure_count}ê°œì— ëŒ€í•´ ë¶„ì„ì„ ì‹œë„í–ˆìŠµë‹ˆë‹¤.\n"
        
        if relevant_reports_found > 0 and requested_items:
             result += f"\nâ€» ìš”ì²­í•˜ì‹  í•­ëª©({', '.join(requested_items)}) ê´€ë ¨ ì •ë³´ê°€ ìˆëŠ” {relevant_reports_found}ê°œì˜ ë³´ê³ ì„œë¥¼ í‘œì‹œí–ˆìŠµë‹ˆë‹¤.\n"

    except Exception as e:
        return f"ì¬ë¬´ ì •ë³´ ê²€ìƒ‰ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n\n{traceback.format_exc()}"

    result += chat_guideline
    return result.strip()


@mcp.tool()
async def search_detailed_financial_data(
    company_name: str,
    start_date: str,
    end_date: str,
    ctx: Context,
    statement_type: Optional[str] = None,
) -> str:
    """
    íšŒì‚¬ì˜ ì„¸ë¶€ì ì¸ ì¬ë¬´ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ë„êµ¬.
    
    Args:
        company_name: íšŒì‚¬ëª… (ì˜ˆ: ì‚¼ì„±ì „ì, ë„¤ì´ë²„ ë“±)
        start_date: ì‹œì‘ì¼ (YYYYMMDD í˜•ì‹, ì˜ˆ: 20250101)
        end_date: ì¢…ë£Œì¼ (YYYYMMDD í˜•ì‹, ì˜ˆ: 20251231)
        ctx: MCP Context ê°ì²´
        statement_type: ì¬ë¬´ì œí‘œ ìœ í˜• ("ì¬ë¬´ìƒíƒœí‘œ", "ì†ìµê³„ì‚°ì„œ", "í˜„ê¸ˆíë¦„í‘œ" ì¤‘ í•˜ë‚˜ ë˜ëŠ” None)
                       Noneì¸ ê²½ìš° ëª¨ë“  ìœ í˜•ì˜ ì¬ë¬´ì œí‘œ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
    Returns:
        ì„ íƒí•œ ì¬ë¬´ì œí‘œ ìœ í˜•(ë“¤)ì˜ ì„¸ë¶€ í•­ëª© ì •ë³´ê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸
    """
    # ê²°ê³¼ ë¬¸ìì—´ ì´ˆê¸°í™”
    result = ""
    api_errors = []
    
    try:
        # ì¬ë¬´ì œí‘œ ìœ í˜• ê²€ì¦
        if statement_type is not None and statement_type not in STATEMENT_TYPES:
            return f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¬ë¬´ì œí‘œ ìœ í˜•ì…ë‹ˆë‹¤. ì§€ì›ë˜ëŠ” ìœ í˜•: {', '.join(STATEMENT_TYPES.keys())}"
        
        # ëª¨ë“  ì¬ë¬´ì œí‘œ ìœ í˜•ì„ ì²˜ë¦¬í•  ê²½ìš°
        if statement_type is None:
            all_statement_types = list(STATEMENT_TYPES.keys())
            await ctx.info(f"{company_name}ì˜ ëª¨ë“  ì¬ë¬´ì œí‘œ ì„¸ë¶€ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.") # await ì¶”ê°€
        else:
            all_statement_types = [statement_type]
            await ctx.info(f"{company_name}ì˜ {statement_type} ì„¸ë¶€ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.") # await ì¶”ê°€
        
        # end_date ì¡°ì •
        original_end_date = end_date
        adjusted_end_date, was_adjusted = adjust_end_date(end_date)
        
        if was_adjusted:
            await ctx.info(f"ê³µì‹œ ì œì¶œ ê¸°ê°„ì„ ê³ ë ¤í•˜ì—¬ ê²€ìƒ‰ ì¢…ë£Œì¼ì„ {original_end_date}ì—ì„œ {adjusted_end_date}ë¡œ ìë™ ì¡°ì •í–ˆìŠµë‹ˆë‹¤.") # await ì¶”ê°€
            end_date = adjusted_end_date
        
        # íšŒì‚¬ ì½”ë“œ ì¡°íšŒ
        corp_code, matched_name = await get_corp_code_by_name(company_name)
        if not corp_code:
            return f"íšŒì‚¬ ê²€ìƒ‰ ì˜¤ë¥˜: {matched_name}"
        
        await ctx.info(f"{matched_name}(ê³ ìœ ë²ˆí˜¸: {corp_code})ì˜ ê³µì‹œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.") # await ì¶”ê°€
        
        # ê³µì‹œ ëª©ë¡ ì¡°íšŒ
        disclosures, error_msg = await get_disclosure_list(corp_code, start_date, end_date)
        if error_msg:
            return error_msg
            
        if not disclosures:
            date_range_msg = f"{start_date}ë¶€í„° {end_date}ê¹Œì§€"
            if was_adjusted:
                date_range_msg += f" (ì›ë˜ ìš”ì²­: {start_date}~{original_end_date}, ê³µì‹œ ì œì¶œ ê¸°ê°„ ê³ ë ¤í•˜ì—¬ í™•ì¥)"
            return f"{date_range_msg} '{matched_name}'(ê³ ìœ ë²ˆí˜¸: {corp_code})ì˜ ì •ê¸°ê³µì‹œê°€ ì—†ìŠµë‹ˆë‹¤."
        
        await ctx.info(f"{len(disclosures)}ê°œì˜ ì •ê¸°ê³µì‹œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. XBRL ë°ì´í„° ì¡°íšŒ ë° ë¶„ì„ì„ ì‹œë„í•©ë‹ˆë‹¤.") # await ì¶”ê°€

        # ê²°ê³¼ ë¬¸ìì—´ ì´ˆê¸°í™”
        result = f"# {matched_name}ì˜ ì„¸ë¶€ ì¬ë¬´ ì •ë³´ ({start_date} ~ {end_date})\n\n"
        
        # ìµœëŒ€ 5ê°œì˜ ê³µì‹œë§Œ ì²˜ë¦¬ (API í˜¸ì¶œ ì œí•œ ë° ì‹œê°„ ê³ ë ¤)
        disclosure_count = min(5, len(disclosures))
        
        # ê° ê³µì‹œë³„ë¡œ XBRL ë°ì´í„° ì¡°íšŒ ë° ì €ì¥
        processed_disclosures = []
        
        for disclosure in disclosures[:disclosure_count]:
            try:
                report_name = disclosure.get('report_nm', 'ì œëª© ì—†ìŒ')
                rcept_dt = disclosure.get('rcept_dt', 'ë‚ ì§œ ì—†ìŒ')
                rcept_no = disclosure.get('rcept_no', '')

                # ë³´ê³ ì„œ ì½”ë“œ ê²°ì •
                reprt_code = determine_report_code(report_name)
                if not rcept_no or not reprt_code:
                    continue

                await ctx.info(f"ê³µì‹œ ë¶„ì„ ì¤‘: {report_name} (ì ‘ìˆ˜ë²ˆí˜¸: {rcept_no})") # await ì¶”ê°€
                
                # XBRL ë°ì´í„° ì¡°íšŒ
                xbrl_text = await get_financial_statement_xbrl(rcept_no, reprt_code)
                
                if not xbrl_text.startswith(("DART API ì˜¤ë¥˜:", "API ìš”ì²­ ì‹¤íŒ¨:", "ZIP íŒŒì¼", "<ì¸ì½”ë”© ì˜¤ë¥˜:")):
                    processed_disclosures.append({
                        'report_name': report_name,
                        'rcept_dt': rcept_dt,
                        'rcept_no': rcept_no,
                        'reprt_code': reprt_code,
                        'xbrl_text': xbrl_text
                    })
                else:
                    error_summary = xbrl_text.split('\n')[0][:100]
                    api_errors.append(f"{report_name}: {error_summary}")
                    await ctx.warning(f"XBRL ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜ ({report_name}): {error_summary}") # ctx.warningì—ë„ await ì¶”ê°€ (ë§Œì•½ ë¹„ë™ê¸°ë¼ë©´)
            except Exception as e:
                api_errors.append(f"{report_name if 'report_name' in locals() else 'ì•Œ ìˆ˜ ì—†ëŠ” ë³´ê³ ì„œ'}: {str(e)}")
                await ctx.error(f"ê³µì‹œ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}") # ctx.errorì—ë„ await ì¶”ê°€ (ë§Œì•½ ë¹„ë™ê¸°ë¼ë©´)
                traceback.print_exc()
        
        # ê° ì¬ë¬´ì œí‘œ ìœ í˜•ë³„ ì²˜ë¦¬
        for current_statement_type in all_statement_types:
            result += f"## {current_statement_type}\n\n"
            
            # í•´ë‹¹ ì¬ë¬´ì œí‘œ ìœ í˜•ì— ëŒ€í•œ íƒœê·¸ ëª©ë¡ ì¡°íšŒ
            items_to_extract = DETAILED_TAGS[current_statement_type]
            
            # ì¬ë¬´ì œí‘œ ìœ í˜•ë³„ ê²°ê³¼ ì €ì¥
            reports_with_data = 0
            
            # ê° ê³µì‹œë³„ ì²˜ë¦¬
            for disclosure in processed_disclosures:
                try:
                    report_name = disclosure['report_name']
                    rcept_dt = disclosure['rcept_dt']
                    rcept_no = disclosure['rcept_no']
                    xbrl_text = disclosure['xbrl_text']
                    
                    # XBRL íŒŒì‹± ë° ë°ì´í„° ì¶”ì¶œ
                    try:
                        financial_data = parse_xbrl_financial_data(xbrl_text, items_to_extract)
                        
                        # ìœ íš¨í•œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸ (ìµœì†Œ 1ê°œ í•­ëª© ì´ìƒ)
                        valid_items_count = sum(1 for value in financial_data.values() 
                                              if value not in INVALID_VALUE_INDICATORS 
                                              and not value.startswith("ì˜¤ë¥˜(") 
                                              and not value.startswith("ë¶„ì„ ì¤‘"))
                        
                        if valid_items_count >= 1:
                            reports_with_data += 1
                            
                            # ë°ì´í„° ê²°ê³¼ì— ì¶”ê°€
                            result += f"### {report_name} ({rcept_dt})\n"
                            result += f"ì ‘ìˆ˜ë²ˆí˜¸: {rcept_no}\n\n"
                            
                            # í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì¶œë ¥
                            result += "| í•­ëª© | ê°’ |\n"
                            result += "|------|------|\n"
                            
                            for item, value in financial_data.items():
                                if value not in INVALID_VALUE_INDICATORS and not value.startswith("ì˜¤ë¥˜(") and not value.startswith("ë¶„ì„ ì¤‘"):
                                    result += f"| {item} | {value} |\n"
                                else:
                                    # ë§¤ì¹­ë˜ì§€ ì•Šì€ í•­ëª©ì€ '-'ë¡œ í‘œì‹œ
                                    result += f"| {item} | - |\n"
                            
                            result += "\n"
                        else:
                            await ctx.info(f"[{report_name}] {current_statement_type}ì˜ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.") # await ì¶”ê°€
                    except Exception as e:
                        await ctx.warning(f"XBRL íŒŒì‹±/ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({report_name}): {e}") # ctx.warningì—ë„ await ì¶”ê°€
                        api_errors.append(f"{report_name} ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                except Exception as e:
                    await ctx.error(f"ê³µì‹œ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}") # ctx.errorì—ë„ await ì¶”ê°€ (ë§Œì•½ ë¹„ë™ê¸°ë¼ë©´)
                    api_errors.append(f"ê³µì‹œ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    traceback.print_exc()
            
            # ì¬ë¬´ì œí‘œ ìœ í˜•ë³„ ê²°ê³¼ ìš”ì•½
            if reports_with_data == 0:
                result += f"ì¡°íšŒëœ ê³µì‹œì—ì„œ ìœ íš¨í•œ {current_statement_type} ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n\n"
            
            result += "-" * 50 + "\n\n"
        
        # ìµœì¢… ê²°ê³¼ ë©”ì‹œì§€ ì¶”ê°€
        if api_errors:
            result += "\n## ì²˜ë¦¬ ì¤‘ ë°œìƒí•œ ì˜¤ë¥˜\n"
            for error in api_errors:
                result += f"- {error}\n"
            result += "\n"
            
        if len(disclosures) > disclosure_count:
            result += f"â€» ì´ {len(disclosures)}ê°œì˜ ì •ê¸°ê³µì‹œ ì¤‘ ìµœì‹  {disclosure_count}ê°œì— ëŒ€í•´ ë¶„ì„ì„ ì‹œë„í–ˆìŠµë‹ˆë‹¤.\n"
        
        if len(processed_disclosures) == 0:
            result += "â€» ëª¨ë“  ê³µì‹œì—ì„œ XBRL ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n"
        
    except Exception as e:
        return f"ì„¸ë¶€ ì¬ë¬´ ì •ë³´ ê²€ìƒ‰ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n\n{traceback.format_exc()}"

    result += chat_guideline
    return result.strip()


@mcp.tool()
async def search_business_information(
    company_name: str,
    start_date: str,
    end_date: str,
    information_type: str,
    ctx: Context,
) -> str:
    """
    íšŒì‚¬ì˜ ì‚¬ì—… ê´€ë ¨ í˜„í™© ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ë„êµ¬
    
    Args:
        company_name: íšŒì‚¬ëª… (ì˜ˆ: ì‚¼ì„±ì „ì, ë„¤ì´ë²„ ë“±)
        start_date: ì‹œì‘ì¼ (YYYYMMDD í˜•ì‹, ì˜ˆ: 20250101)
        end_date: ì¢…ë£Œì¼ (YYYYMMDD í˜•ì‹, ì˜ˆ: 20251231)
        information_type: ì¡°íšŒí•  ì •ë³´ ìœ í˜• 
            'ì‚¬ì—…ì˜ ê°œìš”' - íšŒì‚¬ì˜ ì „ë°˜ì ì¸ ì‚¬ì—… ë‚´ìš©
            'ì£¼ìš” ì œí’ˆ ë° ì„œë¹„ìŠ¤' - íšŒì‚¬ì˜ ì£¼ìš” ì œí’ˆê³¼ ì„œë¹„ìŠ¤ ì •ë³´
            'ì›ì¬ë£Œ ë° ìƒì‚°ì„¤ë¹„' - ì›ì¬ë£Œ ì¡°ë‹¬ ë° ìƒì‚° ì„¤ë¹„ í˜„í™©
            'ë§¤ì¶œ ë° ìˆ˜ì£¼ìƒí™©' - ë§¤ì¶œê³¼ ìˆ˜ì£¼ í˜„í™© ì •ë³´
            'ìœ„í—˜ê´€ë¦¬ ë° íŒŒìƒê±°ë˜' - ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë°©ì•ˆ ë° íŒŒìƒìƒí’ˆ ê±°ë˜ ì •ë³´
            'ì£¼ìš”ê³„ì•½ ë° ì—°êµ¬ê°œë°œí™œë™' - ì£¼ìš” ê³„ì•½ í˜„í™© ë° R&D í™œë™
            'ê¸°íƒ€ ì°¸ê³ ì‚¬í•­' - ê¸°íƒ€ ì‚¬ì—… ê´€ë ¨ ì°¸ê³  ì •ë³´
        ctx: MCP Context ê°ì²´
        
    Returns:
        ìš”ì²­í•œ ì •ë³´ ìœ í˜•ì— ëŒ€í•œ í•´ë‹¹ íšŒì‚¬ì˜ ì‚¬ì—… ì •ë³´ í…ìŠ¤íŠ¸
    """
    # ê²°ê³¼ ë¬¸ìì—´ ì´ˆê¸°í™”
    result = ""
    
    try:
        # ì§€ì›í•˜ëŠ” ì •ë³´ ìœ í˜• ê²€ì¦
        supported_types = [
            'ì‚¬ì—…ì˜ ê°œìš”', 'ì£¼ìš” ì œí’ˆ ë° ì„œë¹„ìŠ¤', 'ì›ì¬ë£Œ ë° ìƒì‚°ì„¤ë¹„',
            'ë§¤ì¶œ ë° ìˆ˜ì£¼ìƒí™©', 'ìœ„í—˜ê´€ë¦¬ ë° íŒŒìƒê±°ë˜', 'ì£¼ìš”ê³„ì•½ ë° ì—°êµ¬ê°œë°œí™œë™',
            'ê¸°íƒ€ ì°¸ê³ ì‚¬í•­'
        ]
        
        if information_type not in supported_types:
            return f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì •ë³´ ìœ í˜•ì…ë‹ˆë‹¤. ì§€ì›ë˜ëŠ” ìœ í˜•: {', '.join(supported_types)}"
        
        # ì§„í–‰ ìƒí™© ì•Œë¦¼
        await ctx.info(f"{company_name}ì˜ {information_type} ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.") # await ì¶”ê°€
        
        # end_date ì¡°ì •
        original_end_date = end_date
        adjusted_end_date, was_adjusted = adjust_end_date(end_date)
        
        if was_adjusted:
            await ctx.info(f"ê³µì‹œ ì œì¶œ ê¸°ê°„ì„ ê³ ë ¤í•˜ì—¬ ê²€ìƒ‰ ì¢…ë£Œì¼ì„ {original_end_date}ì—ì„œ {adjusted_end_date}ë¡œ ìë™ ì¡°ì •í–ˆìŠµë‹ˆë‹¤.") # await ì¶”ê°€
            end_date = adjusted_end_date
        
        # íšŒì‚¬ ì½”ë“œ ì¡°íšŒ
        corp_code, matched_name = await get_corp_code_by_name(company_name)
        if not corp_code:
            return f"íšŒì‚¬ ê²€ìƒ‰ ì˜¤ë¥˜: {matched_name}"
        
        await ctx.info(f"{matched_name}(ê³ ìœ ë²ˆí˜¸: {corp_code})ì˜ ê³µì‹œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.") # await ì¶”ê°€
        
        # ê³µì‹œ ëª©ë¡ ì¡°íšŒ
        disclosures, error_msg = await get_disclosure_list(corp_code, start_date, end_date)
        if error_msg:
            return error_msg
            
        await ctx.info(f"{len(disclosures)}ê°œì˜ ì •ê¸°ê³µì‹œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ì ì ˆí•œ ê³µì‹œë¥¼ ì„ íƒí•˜ì—¬ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.") # await ì¶”ê°€
        
        # ì‚¬ì—…ì •ë³´ë¥¼ í¬í•¨í•  ê°€ëŠ¥ì„±ì´ ë†’ì€ ì •ê¸°ë³´ê³ ì„œë¥¼ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ í•„í„°ë§
        priority_reports = [
            "ì‚¬ì—…ë³´ê³ ì„œ", "ë°˜ê¸°ë³´ê³ ì„œ", "ë¶„ê¸°ë³´ê³ ì„œ"
        ]
        
        selected_disclosure = None
        
        # ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ê³µì‹œ ì„ íƒ
        for priority in priority_reports:
            for disclosure in disclosures:
                report_name = disclosure.get('report_nm', '')
                if priority in report_name:
                    selected_disclosure = disclosure
                    break
            if selected_disclosure:
                break
        
        # ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ê³µì‹œë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ì²« ë²ˆì§¸ ê³µì‹œ ì„ íƒ
        if not selected_disclosure and disclosures:
            selected_disclosure = disclosures[0]
        
        if not selected_disclosure:
            return f"'{matched_name}'ì˜ ì ì ˆí•œ ê³µì‹œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ì„ íƒëœ ê³µì‹œ ì •ë³´
        report_name = selected_disclosure.get('report_nm', 'ì œëª© ì—†ìŒ')
        rcept_dt = selected_disclosure.get('rcept_dt', 'ë‚ ì§œ ì—†ìŒ')
        rcept_no = selected_disclosure.get('rcept_no', '')
        
        await ctx.info(f"'{report_name}' (ì ‘ìˆ˜ì¼: {rcept_no}, ì ‘ìˆ˜ì¼: {rcept_dt}) ê³µì‹œì—ì„œ '{information_type}' ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.") # await ì¶”ê°€
        
        # ì„¹ì…˜ ì¶”ì¶œ
        try:
            section_text = await extract_business_section_from_dart(rcept_no, information_type)
            
            # ì¶”ì¶œ ê²°ê³¼ í™•ì¸
            if section_text.startswith(f"ê³µì‹œì„œë¥˜ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨") or section_text.startswith(f"'{information_type}' ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"):
                api_error = section_text
                result = f"# {matched_name} - {information_type}\n\n"
                result += f"## ì¶œì²˜: {report_name} (ì ‘ìˆ˜ì¼: {rcept_dt})\n\n"
                result += f"ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {api_error}\n\n"
                result += "ë‹¤ìŒê³¼ ê°™ì€ ì´ìœ ë¡œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤:\n"
                result += "1. í•´ë‹¹ ê³µì‹œì— ìš”ì²­í•˜ì‹  ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
                result += "2. DART API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
                result += "3. ì„¹ì…˜ ì¶”ì¶œ ê³¼ì •ì—ì„œ íŒ¨í„´ ë§¤ì¹­ì— ì‹¤íŒ¨í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
                return result
            else:
                # ê²°ê³¼ í¬ë§·íŒ…
                result = f"# {matched_name} - {information_type}\n\n"
                result += f"## ì¶œì²˜: {report_name} (ì ‘ìˆ˜ì¼: {rcept_dt})\n\n"
                result += section_text
                # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ ê²½ìš° ì•ë¶€ë¶„ë§Œ ë°˜í™˜
                max_length = 5000  # ì ì ˆí•œ ìµœëŒ€ ê¸¸ì´ ì„¤ì •
                if len(result) > max_length:
                    result = result[:max_length] + f"\n\n... (ì´í•˜ ìƒëµ, ì´ {len(result)} ì)"
        except Exception as e:
            await ctx.error(f"ì„¹ì…˜ ì¶”ì¶œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}") # ctx.errorì—ë„ await ì¶”ê°€
            result = f"# {matched_name} - {information_type}\n\n"
            result += f"## ì¶œì²˜: {report_name} (ì ‘ìˆ˜ì¼: {rcept_dt})\n\n"
            result += f"ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}\n\n"
            result += "ë‹¤ìŒê³¼ ê°™ì€ ì´ìœ ë¡œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤:\n"
            result += "1. ì„¹ì…˜ ì¶”ì¶œ ê³¼ì •ì—ì„œ ì˜ˆì™¸ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n"
            result += "2. ì˜¤ë¥˜ ìƒì„¸ ì •ë³´: " + traceback.format_exc().replace('\n', '\n   ') + "\n"
            
    except Exception as e:
        return f"ì‚¬ì—… ì •ë³´ ê²€ìƒ‰ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n\n{traceback.format_exc()}"

    result += chat_guideline
    return result.strip()


@mcp.tool()
async def get_current_date(
    ctx: Context = None
) -> str:
    """
    í˜„ì¬ ë‚ ì§œë¥¼ YYYYMMDD í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” ë„êµ¬
    
    Args:
        ctx: MCP Context ê°ì²´ (ì„ íƒ ì‚¬í•­)
        
    Returns:
        YYYYMMDD í˜•ì‹ì˜ í˜„ì¬ ë‚ ì§œ ë¬¸ìì—´
    """
    # í˜„ì¬ ë‚ ì§œë¥¼ YYYYMMDD í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
    formatted_date = datetime.now().strftime("%Y%m%d")
    
    # ì»¨í…ìŠ¤íŠ¸ê°€ ì œê³µëœ ê²½ìš° ë¡œê·¸ ì¶œë ¥
    if ctx:
        await ctx.info(f"í˜„ì¬ ë‚ ì§œ: {formatted_date}") # await ì¶”ê°€
    
    return formatted_date

# --- SWOT ë¶„ì„ ë„êµ¬ ---
@mcp.tool(
    name="swot_analysis",
    description=ENHANCED_SWOT_DESCRIPTION
)
async def swot_analysis(
    thought: str,
    thoughtNumber: int,
    totalThoughts: int,
    nextThoughtNeeded: bool,
    analysisStage: str,
    companyName: str = None,
    jobPosition: str = None,
    industry: str = None,
    isRevision: bool = None,
    revisesThought: int = None,
    branchFromThought: int = None,
    branchId: str = None,
    needsMoreThoughts: bool = None,
    dataSource: str = None,
    languagePreference: str = None,
    autoSearch: bool = True
):
    """í–¥ìƒëœ ê¸°ì—… SWOT ë¶„ì„ì„ ìœ„í•œ ë‹¨ê³„ì  ì‚¬ê³  ë„êµ¬.
    
    Args:
        thought: í˜„ì¬ ë¶„ì„ ë‹¨ê³„ì—ì„œì˜ ìƒê°ì´ë‚˜ í†µì°°
        thoughtNumber: í˜„ì¬ ìƒê° ë²ˆí˜¸
        totalThoughts: ì˜ˆìƒë˜ëŠ” ì´ ìƒê° ìˆ˜
        nextThoughtNeeded: ì¶”ê°€ ìƒê°ì´ í•„ìš”í•œì§€ ì—¬ë¶€
        analysisStage: í˜„ì¬ ë¶„ì„ ë‹¨ê³„ ('planning', 'S', 'W', 'O', 'T', 'synthesis', 'recommendation')
        companyName: ë¶„ì„ ëŒ€ìƒ ê¸°ì—…ëª…
        jobPosition: ì§€ì› ì§ë¬´
        industry: ì‚°ì—… ë¶„ì•¼
        isRevision: ì´ì „ ìƒê°ì„ ìˆ˜ì •í•˜ëŠ”ì§€ ì—¬ë¶€
        revisesThought: ìˆ˜ì • ëŒ€ìƒ ìƒê° ë²ˆí˜¸
        branchFromThought: ë¶„ê¸° ì‹œì‘ì  ìƒê° ë²ˆí˜¸
        branchId: ë¶„ê¸° ì‹ë³„ì
        needsMoreThoughts: ì¶”ê°€ ìƒê°ì´ í•„ìš”í•œì§€ ì—¬ë¶€
        dataSource: ì •ë³´ ì¶œì²˜
        languagePreference: ì–¸ì–´ ì„¤ì • (ko: í•œêµ­ì–´, en: ì˜ì–´)
        autoSearch: ìë™ ê²€ìƒ‰ ìˆ˜í–‰ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
    
    Returns:
        Dict with thought processing results
    """
    input_data = {
        "thought": thought,
        "thoughtNumber": thoughtNumber,
        "totalThoughts": totalThoughts,
        "nextThoughtNeeded": nextThoughtNeeded,
        "analysisStage": analysisStage
    }
    
    # ì˜µì…˜ íŒŒë¼ë¯¸í„° ì¶”ê°€
    if companyName is not None:
        input_data["companyName"] = companyName
    if jobPosition is not None:
        input_data["jobPosition"] = jobPosition
    if industry is not None:
        input_data["industry"] = industry
    if isRevision is not None:
        input_data["isRevision"] = isRevision
    if revisesThought is not None:
        input_data["revisesThought"] = revisesThought
    if branchFromThought is not None:
        input_data["branchFromThought"] = branchFromThought
    if branchId is not None:
        input_data["branchId"] = branchId
    if needsMoreThoughts is not None:
        input_data["needsMoreThoughts"] = needsMoreThoughts
    if dataSource is not None:
        input_data["dataSource"] = dataSource
    if languagePreference is not None:
        input_data["languagePreference"] = languagePreference
    
    # ìë™ ê²€ìƒ‰ ìˆ˜í–‰
    if autoSearch and companyName:
        try:
            search_result = await perform_stage_based_search(analysisStage, companyName, jobPosition, industry)
            if search_result:
                # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì‚¬ê³  ë‚´ìš© ì•ì— ì¶”ê°€
                input_data["thought"] = f"[ìë™ ê²€ìƒ‰ ê²°ê³¼]\n{search_result}\n\n[ì‚¬ìš©ì ë¶„ì„]\n{thought}"
                input_data["dataSource"] = "ìë™ ê²€ìƒ‰ + ì‚¬ìš©ì ì…ë ¥"
        except Exception as e:
            logger.error(f"ìë™ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    return swot_server.process_thought(input_data)

async def perform_stage_based_search(stage: str, company_name: str, job_position: str = None, industry: str = None) -> str:
    """
    SWOT ë¶„ì„ ë‹¨ê³„ì— ë”°ë¼ ì ì ˆí•œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        stage: ë¶„ì„ ë‹¨ê³„ ('planning', 'S', 'W', 'O', 'T', 'synthesis', 'recommendation')
        company_name: íšŒì‚¬ëª…
        job_position: ì§ë¬´ëª… (ì„ íƒ ì‚¬í•­)
        industry: ì‚°ì—… ë¶„ì•¼ (ì„ íƒ ì‚¬í•­)
    
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë¬¸ìì—´
    """
    search_results = []
    
    # í˜„ì¬ ë‚ ì§œ êµ¬í•˜ê¸° (ì¬ë¬´ ì •ë³´ ê²€ìƒ‰ìš©)
    current_date = await get_current_date()
    # 1ë…„ ì „ ë‚ ì§œ ê³„ì‚°
    year = int(current_date[:4])
    one_year_ago = f"{year-1}{current_date[4:]}"
    
    try:
        if stage == 'planning':
            # ê¸°ì—… ê°œìš” ë° ê¸°ë³¸ ì •ë³´ ê²€ìƒ‰
            search_results.append(await search_webkr(f"{company_name} ê¸°ì—… ê°œìš”", display=5))
            search_results.append(await search_google(f"{company_name} ê¸°ì—… ê°œìš”", num_results=5))
            
            # ìµœê·¼ ë‰´ìŠ¤ ê²€ìƒ‰
            search_results.append(await search_news(f"{company_name} ìµœì‹ ", display=3, sort="date"))
            
        elif stage == 'S':  # ê°•ì  ë¶„ì„
            # ê¸°ì—… ê°•ì  ê´€ë ¨ ê²€ìƒ‰
            search_results.append(await search_webkr(f"{company_name} ê°•ì  ê²½ìŸë ¥", display=5))
            search_results.append(await search_google(f"{company_name} ê°•ì  ê²½ìŸë ¥", num_results=5))
            
            # ì¬ë¬´ ì •ë³´ ê²€ìƒ‰ - ìì‚°, ë§¤ì¶œ ë“±
            try:
                financial_data = await search_disclosure(company_name, one_year_ago, current_date, 
                                                         requested_items=["ë§¤ì¶œì•¡", "ì˜ì—…ì´ìµ", "ë‹¹ê¸°ìˆœì´ìµ"])
                search_results.append(financial_data)
            except Exception as e:
                pass
                
            # ì œí’ˆ ë° ì„œë¹„ìŠ¤ ì •ë³´
            try:
                business_info = await search_business_information(company_name, one_year_ago, current_date, 
                                                                 "ì£¼ìš” ì œí’ˆ ë° ì„œë¹„ìŠ¤")
                search_results.append(business_info)
            except Exception as e:
                pass
            
        elif stage == 'W':  # ì•½ì  ë¶„ì„
            # ê¸°ì—… ì•½ì  ê´€ë ¨ ê²€ìƒ‰
            search_results.append(await search_webkr(f"{company_name} ì•½ì  ë¬¸ì œì ", display=5))
            search_results.append(await search_google(f"{company_name} ì•½ì  ë¬¸ì œì ", num_results=5))
            
            # ê²½ìŸì‚¬ ë¹„êµ ê²€ìƒ‰
            if industry:
                search_results.append(await search_webkr(f"{company_name} {industry} ê²½ìŸì‚¬ ë¹„êµ", display=5))
                search_results.append(await search_google(f"{company_name} {industry} ê²½ìŸì‚¬ ë¹„êµ", num_results=5))
            
        elif stage == 'O':  # ê¸°íšŒ ë¶„ì„
            # ì‚°ì—… íŠ¸ë Œë“œ ê²€ìƒ‰
            if industry:
                search_results.append(await search_news(f"{industry} íŠ¸ë Œë“œ ì „ë§", display=3, sort="date"))
            
            # ê¸°ì—… ê¸°íšŒ ìš”ì¸ ê²€ìƒ‰
            search_results.append(await search_webkr(f"{company_name} ê¸°íšŒ ì„±ì¥", display=5))
            search_results.append(await search_google(f"{company_name} ê¸°íšŒ ì„±ì¥", num_results=5))
            
        elif stage == 'T':  # ìœ„í˜‘ ë¶„ì„
            # ìœ„í—˜ ìš”ì†Œ ê²€ìƒ‰
            search_results.append(await search_webkr(f"{company_name} ìœ„í˜‘ ë¦¬ìŠ¤í¬", display=5))
            search_results.append(await search_google(f"{company_name} ìœ„í˜‘ ë¦¬ìŠ¤í¬", num_results=5))
            
            # ì‚°ì—… ìœ„í˜‘ ìš”ì†Œ ê²€ìƒ‰
            if industry:
                search_results.append(await search_news(f"{industry} ìœ„ê¸° ê·œì œ", display=3, sort="date"))

        elif stage == 'synthesis':  # ì¢…í•© ë¶„ì„
            # SWOT ì¢…í•© ë¶„ì„ ê²€ìƒ‰
            search_results.append(await search_swot_webkr(company_name, display=5))
            search_results.append(await search_google(f"{company_name} SWOT ë¶„ì„", num_results=5))
            
        elif stage == 'recommendation':  # ì§€ì› ì „ëµ
            # ì±„ìš© ì •ë³´ ê²€ìƒ‰
            if job_position:
                search_results.append(await search_webkr(f"{company_name} {job_position} ì±„ìš© ìê²©", display=5))
                search_results.append(await search_google(f"{company_name} {job_position} ì±„ìš© ìê²©", num_results=5))

            # ê¸°ì—… ë¬¸í™” ê²€ìƒ‰
            search_results.append(await search_webkr(f"{company_name} ê¸°ì—…ë¬¸í™” ì¡°ì§ë¬¸í™”", display=5))
            search_results.append(await search_google(f"{company_name} ê¸°ì—…ë¬¸í™” ì¡°ì§ë¬¸í™”", num_results=5))
    
    except Exception as e:
        return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ë° ì •ë¦¬
    summarized_results = []
    for result in search_results:
        if result and isinstance(result, str):
            # ë„ˆë¬´ ê¸´ ê²°ê³¼ ìš”ì•½
            if len(result) > 1500:
                result = result[:1500] + "...(ì¤‘ëµ)..."
            summarized_results.append(result)
    
    return "\n\n---\n\n".join(summarized_results)

# SWOT ë¶„ì„ì„ ìœ„í•œ ì¶”ê°€ ë„êµ¬
@mcp.tool()
async def search_company_swot(company_name: str) -> str:
    """
    íŠ¹ì • ê¸°ì—…ì˜ SWOT ë¶„ì„ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ì¢…í•©í•˜ëŠ” ë„êµ¬
    
    Args:
        company_name: ê¸°ì—…ëª…
    
    Returns:
        SWOT ë¶„ì„ ì¢…í•© ì •ë³´
    """
    search_results = []
    
    try:
        # í˜„ì¬ ë‚ ì§œ êµ¬í•˜ê¸°
        current_date = await get_current_date()
        # 1ë…„ ì „ ë‚ ì§œ ê³„ì‚°
        year = int(current_date[:4])
        one_year_ago = f"{year-1}{current_date[4:]}"
        
        # 1. ê¸°ì—… SWOT ë¶„ì„ ì›¹ ê²€ìƒ‰
        swot_search = await search_swot_webkr(company_name, display=3)
        search_results.append(f"## SWOT ë¶„ì„ ê²€ìƒ‰ ê²°ê³¼\n{swot_search}")
        
        # 2. ìµœì‹  ë‰´ìŠ¤ ê²€ìƒ‰
        news_search = await search_news(f"{company_name} ìµœì‹ ", display=3, sort="date")
        search_results.append(f"## ìµœì‹  ë‰´ìŠ¤\n{news_search}")
        
        # 3. ì¬ë¬´ ì •ë³´ ê²€ìƒ‰
        try:
            financial_data = await search_disclosure(company_name, one_year_ago, current_date, 
                                                   requested_items=["ë§¤ì¶œì•¡", "ì˜ì—…ì´ìµ", "ë‹¹ê¸°ìˆœì´ìµ"])
            search_results.append(f"## ì¬ë¬´ ì •ë³´\n{financial_data}")
        except Exception as e:
            search_results.append(f"## ì¬ë¬´ ì •ë³´\nì¬ë¬´ì •ë³´ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        
        # 4. ì œí’ˆ ë° ì„œë¹„ìŠ¤ ì •ë³´
        try:
            business_info = await search_business_information(company_name, one_year_ago, current_date, 
                                                           "ì£¼ìš” ì œí’ˆ ë° ì„œë¹„ìŠ¤")
            # ë„ˆë¬´ ê¸´ ë‚´ìš© ìš”ì•½
            if len(business_info) > 1500:
                business_info = business_info[:1500] + "...(ì¤‘ëµ)..."
            search_results.append(f"## ì œí’ˆ ë° ì„œë¹„ìŠ¤ ì •ë³´\n{business_info}")
        except Exception as e:
            search_results.append(f"## ì œí’ˆ ë° ì„œë¹„ìŠ¤ ì •ë³´\nì •ë³´ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        
        # ê²°ê³¼ ì¢…í•© ë° ë°˜í™˜
        return "\n\n---\n\n".join(search_results)
    
    except Exception as e:
        return f"SWOT ë¶„ì„ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# --- ë„¤ì´ë²„ ê²€ìƒ‰ MCP ë„êµ¬ ---
@mcp.tool()
async def search_blog(query: str, display: int = 10, page: int = 1, sort: str = "sim") -> str:
    """
    ë¸”ë¡œê·¸ ê²€ìƒ‰ ë„êµ¬
    
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ í‚¤ì›Œë“œì— ëŒ€í•œ ë¸”ë¡œê·¸ ê¸€ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    Args:
        query: ê²€ìƒ‰ì–´
        display: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜ (ìµœëŒ€ 100)
        page: ê²°ê³¼ í˜ì´ì§€ ë²ˆí˜¸
        sort: ì •ë ¬ ë°©ì‹ (sim: ìœ ì‚¬ë„ìˆœ, date: ë‚ ì§œìˆœ)
        
    Returns:
        str: ê²€ìƒ‰ ê²°ê³¼ ë¬¸ìì—´
    """
    # ì‹œì‘ ìœ„ì¹˜ ê³„ì‚°
    start = calculate_start(page, display)
    
    # API í˜¸ì¶œì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ì„¤ì •
    params = {
        "query": query,
        "display": display,
        "start": start,
        "sort": sort
    }
    
    # API í˜¸ì¶œ ë° ê²°ê³¼ ë°˜í™˜
    return await _make_naver_api_call("blog.json", params, BlogResult, "ë¸”ë¡œê·¸")

@mcp.tool()
async def search_news(query: str, display: int = 10, page: int = 1, sort: str = "sim") -> str:
    """
    ë‰´ìŠ¤ ê²€ìƒ‰ ë„êµ¬
    
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ í‚¤ì›Œë“œì— ëŒ€í•œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    Args:
        query: ê²€ìƒ‰ì–´
        display: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜ (ìµœëŒ€ 100)
        page: ê²°ê³¼ í˜ì´ì§€ ë²ˆí˜¸
        sort: ì •ë ¬ ë°©ì‹ (sim: ìœ ì‚¬ë„ìˆœ, date: ë‚ ì§œìˆœ)
        
    Returns:
        str: ê²€ìƒ‰ ê²°ê³¼ ë¬¸ìì—´
    """
    # ì‹œì‘ ìœ„ì¹˜ ê³„ì‚°
    start = calculate_start(page, display)
    
    # API í˜¸ì¶œì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ì„¤ì •
    params = {
        "query": query,
        "display": display,
        "start": start,
        "sort": sort
    }
    
    # API í˜¸ì¶œ ë° ê²°ê³¼ ë°˜í™˜
    return await _make_naver_api_call("news.json", params, NewsResult, "ë‰´ìŠ¤")

# @mcp.tool()
# async def search_book(query: str, display: int = 10, page: int = 1, sort: str = "sim") -> str:
#     """
#     ë„ì„œ ê²€ìƒ‰ ë„êµ¬
    
#     ë„¤ì´ë²„ ë„ì„œ ê²€ìƒ‰ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ í‚¤ì›Œë“œì— ëŒ€í•œ ë„ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
#     Args:
#         query: ê²€ìƒ‰ì–´
#         display: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜ (ìµœëŒ€ 100)
#         page: ê²°ê³¼ í˜ì´ì§€ ë²ˆí˜¸
#         sort: ì •ë ¬ ë°©ì‹ (sim: ìœ ì‚¬ë„ìˆœ, date: ì¶œê°„ì¼ìˆœ)
        
#     Returns:
#         str: ê²€ìƒ‰ ê²°ê³¼ ë¬¸ìì—´
#     """
#     # ì‹œì‘ ìœ„ì¹˜ ê³„ì‚°
#     start = calculate_start(page, display)
    
#     # API í˜¸ì¶œì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ì„¤ì •
#     params = {
#         "query": query,
#         "display": display,
#         "start": start,
#         "sort": sort
#     }
    
#     # API í˜¸ì¶œ ë° ê²°ê³¼ ë°˜í™˜
#     return await _make_naver_api_call("book.json", params, BookResult, "ë„ì„œ")

# @mcp.tool()
# async def search_encyclopedia(query: str, display: int = 10, page: int = 1, sort: str = "sim") -> str:
#     """
#     ë°±ê³¼ì‚¬ì „ ê²€ìƒ‰ ë„êµ¬
    
#     ë„¤ì´ë²„ ë°±ê³¼ì‚¬ì „ ê²€ìƒ‰ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ í‚¤ì›Œë“œì— ëŒ€í•œ ë°±ê³¼ì‚¬ì „ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
#     Args:
#         query: ê²€ìƒ‰ì–´
#         display: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜ (ìµœëŒ€ 100)
#         page: ê²°ê³¼ í˜ì´ì§€ ë²ˆí˜¸
#         sort: ì •ë ¬ ë°©ì‹ (sim: ìœ ì‚¬ë„ìˆœ, date: ë‚ ì§œìˆœ)
        
#     Returns:
#         str: ê²€ìƒ‰ ê²°ê³¼ ë¬¸ìì—´
#     """
#     # ì‹œì‘ ìœ„ì¹˜ ê³„ì‚°
#     start = calculate_start(page, display)
    
#     # API í˜¸ì¶œì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ì„¤ì •
#     params = {
#         "query": query,
#         "display": display,
#         "start": start,
#         "sort": sort
#     }
    
#     # API í˜¸ì¶œ ë° ê²°ê³¼ ë°˜í™˜
#     return await _make_naver_api_call("encyc.json", params, EncycResult, "ë°±ê³¼ì‚¬ì „")

@mcp.tool()
async def search_cafe_article(query: str, display: int = 10, page: int = 1, sort: str = "sim") -> str:
    """
    ì¹´í˜ ê¸€ ê²€ìƒ‰ ë„êµ¬
    
    ë„¤ì´ë²„ ì¹´í˜ ê¸€ ê²€ìƒ‰ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ í‚¤ì›Œë“œì— ëŒ€í•œ ì¹´í˜ ê¸€ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    Args:
        query: ê²€ìƒ‰ì–´
        display: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜ (ìµœëŒ€ 100)
        page: ê²°ê³¼ í˜ì´ì§€ ë²ˆí˜¸
        sort: ì •ë ¬ ë°©ì‹ (sim: ìœ ì‚¬ë„ìˆœ, date: ë‚ ì§œìˆœ)
        
    Returns:
        str: ê²€ìƒ‰ ê²°ê³¼ ë¬¸ìì—´
    """
    # ì‹œì‘ ìœ„ì¹˜ ê³„ì‚°
    start = calculate_start(page, display)
    
    # API í˜¸ì¶œì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ì„¤ì •
    params = {
        "query": query,
        "display": display,
        "start": start,
        "sort": sort
    }
    
    # API í˜¸ì¶œ ë° ê²°ê³¼ ë°˜í™˜
    return await _make_naver_api_call("cafearticle.json", params, CafeArticleResult, "ì¹´í˜ ê¸€")

@mcp.tool()
async def search_kin(query: str, display: int = 10, page: int = 1, sort: str = "sim") -> str:
    """
    ì§€ì‹iN ê²€ìƒ‰ ë„êµ¬
    
    ë„¤ì´ë²„ ì§€ì‹iN ê²€ìƒ‰ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ í‚¤ì›Œë“œì— ëŒ€í•œ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    Args:
        query: ê²€ìƒ‰ì–´
        display: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜ (ìµœëŒ€ 100)
        page: ê²°ê³¼ í˜ì´ì§€ ë²ˆí˜¸
        sort: ì •ë ¬ ë°©ì‹ (sim: ìœ ì‚¬ë„ìˆœ, date: ë‚ ì§œìˆœ, point: ì¡°íšŒìˆœ)
        
    Returns:
        str: ê²€ìƒ‰ ê²°ê³¼ ë¬¸ìì—´
    """
    # ì‹œì‘ ìœ„ì¹˜ ê³„ì‚°
    start = calculate_start(page, display)
    
    # API í˜¸ì¶œì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ì„¤ì •
    params = {
        "query": query,
        "display": display,
        "start": start,
        "sort": sort
    }
    
    # API í˜¸ì¶œ ë° ê²°ê³¼ ë°˜í™˜
    return await _make_naver_api_call("kin.json", params, KinResult, "ì§€ì‹iN")

@mcp.tool()
async def search_webkr(query: str, display: int = 10, page: int = 1) -> str:
    """
    ì›¹ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬
    
    ë„¤ì´ë²„ ì›¹ë¬¸ì„œ ê²€ìƒ‰ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ í‚¤ì›Œë“œì— ëŒ€í•œ ì›¹ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    Args:
        query: ê²€ìƒ‰ì–´
        display: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜ (ìµœëŒ€ 100)
        page: ê²°ê³¼ í˜ì´ì§€ ë²ˆí˜¸
        
    Returns:
        str: ê²€ìƒ‰ ê²°ê³¼ ë¬¸ìì—´
    """
    # ì‹œì‘ ìœ„ì¹˜ ê³„ì‚°
    start = calculate_start(page, display)
    
    # API í˜¸ì¶œì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ì„¤ì •
    params = {
        "query": query,
        "display": display,
        "start": start
    }
    
    # API í˜¸ì¶œ ë° ê²°ê³¼ ë°˜í™˜
    return await _make_naver_api_call("webkr.json", params, WebkrResult, "ì›¹ë¬¸ì„œ")

@mcp.tool(
    name="search_swot_webkr",
    description="Searches for SWOT analysis of a given company using the given company name. The page parameter allows for page navigation."
)
async def search_swot_webkr(company_name: str, display: int = 10, page: int = 1, sort: str = "date") -> str:
    """
    ê¸°ì—… SWOT ë¶„ì„ ì›¹ ê²€ìƒ‰ ë„êµ¬
    
    ì£¼ì–´ì§„ íšŒì‚¬ëª…ì— ëŒ€í•œ SWOT ë¶„ì„ ì •ë³´ë¥¼ ì›¹ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    Args:
        company_name: íšŒì‚¬ëª…
        display: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜ (ìµœëŒ€ 100)
        page: ê²°ê³¼ í˜ì´ì§€ ë²ˆí˜¸
        sort: ì •ë ¬ ë°©ì‹ (sim: ìœ ì‚¬ë„ìˆœ, date: ë‚ ì§œìˆœ)
        
    Returns:
        str: ê²€ìƒ‰ ê²°ê³¼ ë¬¸ìì—´
    """
    # ê²€ìƒ‰ì–´ ìƒì„± (íšŒì‚¬ëª… + SWOT ë¶„ì„)
    query = f"{company_name} SWOT ë¶„ì„"
    
    # ì‹œì‘ ìœ„ì¹˜ ê³„ì‚°
    start = calculate_start(page, display)
    
    # API í˜¸ì¶œì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ì„¤ì •
    params = {
        "query": query,
        "display": display,
        "start": start,
        "sort": sort
    }
    
    # API í˜¸ì¶œ ë° ê²°ê³¼ ë°˜í™˜
    return await _make_naver_api_call("webkr.json", params, WebkrResult, "SWOT ë¶„ì„ ì›¹ë¬¸ì„œ")

# URL ê°€ì ¸ì˜¤ê¸° ê´€ë ¨ ìƒìˆ˜
DEFAULT_USER_AGENT = "ModelContextProtocol/1.0 (CompanyAnalysis; +https://github.com/modelcontextprotocol/servers)"

def determine_report_code(report_name: str) -> Optional[str]:
    """
    ë³´ê³ ì„œ ì´ë¦„ìœ¼ë¡œ ë³´ê³ ì„œ ì½”ë“œ ê²°ì •
    
    Args:
        report_name: ë³´ê³ ì„œ ì´ë¦„
    
    Returns:
        í•´ë‹¹í•˜ëŠ” ë³´ê³ ì„œ ì½”ë“œ ë˜ëŠ” None
    """
    if "ì‚¬ì—…ë³´ê³ ì„œ" in report_name:
        return REPORT_CODE["ì‚¬ì—…ë³´ê³ ì„œ"]
    elif "ë°˜ê¸°ë³´ê³ ì„œ" in report_name:
        return REPORT_CODE["ë°˜ê¸°ë³´ê³ ì„œ"]
    elif "ë¶„ê¸°ë³´ê³ ì„œ" in report_name:
        if ".03)" in report_name or "(1ë¶„ê¸°)" in report_name:
            return REPORT_CODE["1ë¶„ê¸°ë³´ê³ ì„œ"]
        elif ".09)" in report_name or "(3ë¶„ê¸°)" in report_name:
            return REPORT_CODE["3ë¶„ê¸°ë³´ê³ ì„œ"]
    
    return None


def adjust_end_date(end_date: str) -> Tuple[str, bool]:
    """
    ê³µì‹œ ì œì¶œ ê¸°ê°„ì„ ê³ ë ¤í•˜ì—¬ ì¢…ë£Œì¼ ì¡°ì •
    
    Args:
        end_date: ì›ë˜ ì¢…ë£Œì¼ (YYYYMMDD)
    
    Returns:
        ì¡°ì •ëœ ì¢…ë£Œì¼ê³¼ ì¡°ì • ì—¬ë¶€
    """
    try:
        # ì…ë ¥ëœ end_dateë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜
        end_date_obj = datetime.strptime(end_date, "%Y%m%d")
        
        # í˜„ì¬ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
        current_date = datetime.now()
        
        # end_dateê°€ í˜„ì¬ ë‚ ì§œë³´ë‹¤ ê³¼ê±°ì¸ ê²½ìš° í˜„ì¬ ë‚ ì§œë¡œ ì¡°ì •
        if end_date_obj < current_date:
            end_date_obj = current_date
        
        # 95ì¼ ì¶”ê°€
        adjusted_end_date_obj = end_date_obj + timedelta(days=95)
        
        # í˜„ì¬ ë‚ ì§œë³´ë‹¤ ë¯¸ë˜ì¸ ê²½ìš° í˜„ì¬ ë‚ ì§œë¡œ ì¡°ì •
        if adjusted_end_date_obj > current_date:
            adjusted_end_date_obj = current_date
        
        # ë‹¤ì‹œ ë¬¸ìì—´ë¡œ ë³€í™˜
        adjusted_end_date = adjusted_end_date_obj.strftime("%Y%m%d")
        
        # ì¡°ì • ì—¬ë¶€ ë°˜í™˜
        return adjusted_end_date, adjusted_end_date != end_date
    except Exception as e:
        logger.error(f"ë‚ ì§œ ì¡°ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return end_date, False

# ===== URL ê°€ì ¸ì˜¤ê¸° ê´€ë ¨ ì½”ë“œ =====

def detect_namespaces(xbrl_content: str, base_namespaces: Dict[str, str]) -> Dict[str, str]:
    """
    XBRL ë¬¸ì„œì—ì„œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ ì¶”ì¶œí•˜ê³  ê¸°ë³¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì™€ ë³‘í•©
    
    Args:
        xbrl_content: XBRL ë¬¸ì„œ ë‚´ìš©
        base_namespaces: ê¸°ë³¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë”•ì…”ë„ˆë¦¬
    
    Returns:
        ì—…ë°ì´íŠ¸ëœ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë”•ì…”ë„ˆë¦¬
    """
    namespaces = base_namespaces.copy()
    detected = {}
    
    try:
        for event, node in ET.iterparse(StringIO(xbrl_content), events=['start-ns']):
            prefix, uri = node
            if prefix and prefix not in namespaces:
                namespaces[prefix] = uri
                detected[prefix] = uri
            elif prefix and namespaces.get(prefix) != uri:
                namespaces[prefix] = uri
                detected[prefix] = uri
    except Exception:
        pass  # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê°ì§€ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
    
    return namespaces, detected


def extract_fiscal_year(context_refs: Set[str]) -> str:
    """
    contextRef ì§‘í•©ì—ì„œ íšŒê³„ì—°ë„ ì¶”ì¶œ
    
    Args:
        context_refs: XBRL ë¬¸ì„œì—ì„œ ì¶”ì¶œí•œ contextRef ì§‘í•©
    
    Returns:
        ê°ì§€ëœ íšŒê³„ì—°ë„ ë˜ëŠ” í˜„ì¬ ì—°ë„
    """
    for context_ref in context_refs:
        if 'CFY' in context_ref and len(context_ref) > 7:
            match = re.search(r'CFY(\d{4})', context_ref)
            if match:
                return match.group(1)
    
    # íšŒê³„ì—°ë„ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°, í˜„ì¬ ì—°ë„ë¥¼ ì‚¬ìš©
    return str(datetime.now().year)


def get_pattern_by_item_type(item_name: str) -> Dict[str, str]:
    """
    í•­ëª© ìœ í˜•ì— ë”°ë¥¸ ì ì ˆí•œ íŒ¨í„´ ì„ íƒ
    
    Args:
        item_name: ì¬ë¬´ í•­ëª© ì´ë¦„
    
    Returns:
        í•­ëª© ìœ í˜•ì— ë§ëŠ” íŒ¨í„´ ë”•ì…”ë„ˆë¦¬
    """
    # í˜„ê¸ˆíë¦„í‘œ í•­ëª© í™•ì¸
    if item_name in CASH_FLOW_ITEMS or item_name in DETAILED_TAGS["í˜„ê¸ˆíë¦„í‘œ"]:
        return CASH_FLOW_PATTERNS
    
    # ì¬ë¬´ìƒíƒœí‘œ í•­ëª© í™•ì¸
    elif item_name in BALANCE_SHEET_ITEMS or item_name in DETAILED_TAGS["ì¬ë¬´ìƒíƒœí‘œ"]:
        return BALANCE_SHEET_PATTERNS
    
    # ì†ìµê³„ì‚°ì„œ í•­ëª© (ê¸°ë³¸ê°’)
    else:
        return REPORT_PATTERNS


def format_numeric_value(value_text: str, decimals: str) -> str:
    """
    XBRL ìˆ«ì ê°’ì„ ë³´ê¸° ì¢‹ì€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    
    Args:
        value_text: XBRL í•­ëª©ì˜ ê°’
        decimals: ì†Œìˆ˜ì  ìë¦¬ìˆ˜ ì„¤ì •
    
    Returns:
        í¬ë§·íŒ…ëœ ê°’
    """
    try:
        value = float(value_text)
        
        # ë§¤ìš° í° ìˆ«ìëŠ” ë¬¸ìì—´ë¡œ ì²˜ë¦¬í•˜ì—¬ ë°˜í™˜ (í° ìˆ«ì ì²˜ë¦¬ì‹œ ì˜¤ë²„í”Œë¡œìš° ë°©ì§€)
        if abs(value) > 1e14:  # 100ì¡° ì´ìƒì˜ í° ìˆ«ì
            # ë¬¸ìì—´ í˜•íƒœë¡œ í‘œì‹œí•˜ì—¬ ìˆ«ì ê·¸ëŒ€ë¡œ ë°˜í™˜
            if decimals and int(decimals) > 0:
                return value_text
            else:
                # ì •ìˆ˜ ë¶€ë¶„ë§Œ í‘œì‹œ
                return f"{int(value):,}"
        
        # ì†Œìˆ˜ì  ì²˜ë¦¬
        if decimals and int(decimals) > 0:
            format_str = f"{{:,.{abs(int(decimals))}f}}"
            return format_str.format(value)
        else:
            # ë¶€ë™ì†Œìˆ˜ì  ì •ë°€ë„ ë¬¸ì œ ë°©ì§€ë¥¼ ìœ„í•´ ì •ìˆ˜ë¡œ ë³€í™˜ í›„ ì½¤ë§ˆ ì¶”ê°€
            return f"{int(value):,}"
    except (ValueError, TypeError):
        return value_text  # ë³€í™˜í•  ìˆ˜ ì—†ëŠ” ê²½ìš° ì›ë³¸ ë°˜í™˜


def parse_xbrl_financial_data(xbrl_content: str, items_and_tags: Dict[str, List[str]]) -> Dict[str, str]:
    """
    XBRL ë¬¸ì„œì˜ íŠ¹ì • ì¬ë¬´ í•­ëª©ë“¤ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        xbrl_content: XBRL ë¬¸ì„œ ë‚´ìš©
        items_and_tags: ì¶”ì¶œí•  ì¬ë¬´ í•­ëª© ì´ë¦„ê³¼ í•´ë‹¹ XBRL íƒœê·¸ ëª©ë¡ì˜ ë”•ì…”ë„ˆë¦¬
            ì˜ˆ: {"ë§¤ì¶œì•¡": ["ifrs-full:Revenue"]}
    
    Returns:
        ì¬ë¬´ í•­ëª© ì´ë¦„ê³¼ ì¶”ì¶œëœ ê°’ì˜ ë”•ì…”ë„ˆë¦¬
    """
    if not xbrl_content or not items_and_tags:
        return {"ì˜¤ë¥˜": "ì…ë ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
    
    # ê²°ê³¼ ì´ˆê¸°í™”
    result = {}
    
    try:
        # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê¸°ë³¸ê°’
        base_namespaces = {
            "xbrli": "http://www.xbrl.org/2003/instance",
            "dart": "http://dart.fss.or.kr/xbrl",
            "ifrs-full": "http://xbrl.ifrs.org/taxonomy/",
            "ko-gaap": "http://dart.fss.or.kr/",
        }
        
        # XBRL ë¬¸ì„œì—ì„œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê²€ì¶œ
        namespaces, detected = detect_namespaces(xbrl_content, base_namespaces)
        
        # contextRef ìºì‹œ (ë™ì¼ íŒ¨í„´ ê²€ìƒ‰ ì¬ì‚¬ìš©)
        context_ref_cache = {}
        
        # ë¬¸ì„œ ë‚´ ëª¨ë“  í•­ëª©ì— ëŒ€í•œ contextRef ì§‘í•© ìˆ˜ì§‘
        all_context_refs = set()
        for match in re.finditer(r'contextRef="([^"]+)"', xbrl_content):
            all_context_refs.add(match.group(1))
        
        # ë¬¸ì„œ ë‚´ íšŒê³„ì—°ë„ ì •ë³´ ì¶”ì¶œ ì‹œë„
        fiscal_year = extract_fiscal_year(all_context_refs)
        
        # ê° ìš”ì²­ í•­ëª©ì— ëŒ€í•´ ì²˜ë¦¬
        for item_name, tag_list in items_and_tags.items():
            item_value = "N/A"  # ê¸°ë³¸ê°’
            found = False
            
            # í•­ëª© ìœ í˜•ì— ë§ëŠ” íŒ¨í„´ ì„ íƒ
            pattern_by_type = get_pattern_by_item_type(item_name)
            
            # ì „ì²´ íƒœê·¸ì— ëŒ€í•´ ê²€ìƒ‰
            for tag in tag_list:
                # í•­ëª©ì„ ì°¾ê¸° ìœ„í•œ ê°€ëŠ¥í•œ ëª¨ë“  íŒ¨í„´ ì¡°í•©
                for period_type, pattern in pattern_by_type.items():
                    # ìºì‹œëœ context_refê°€ ìˆìœ¼ë©´ ì¬ì‚¬ìš©
                    context_cache_key = f"{period_type}_{fiscal_year}"
                    
                    if context_cache_key in context_ref_cache:
                        matching_contexts = context_ref_cache[context_cache_key]
                    else:
                        # í•´ë‹¹ íŒ¨í„´ì— ë§ëŠ” ëª¨ë“  contextRef ìˆ˜ì§‘
                        pattern_regex = f"{pattern}{fiscal_year}"
                        matching_contexts = [ref for ref in all_context_refs if pattern_regex in ref]
                        context_ref_cache[context_cache_key] = matching_contexts
                    
                    if not matching_contexts:
                        continue
                    
                    # íƒœê·¸ì— ëŒ€í•œ ì •ê·œì‹ íŒ¨í„´ (ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê³ ë ¤)
                    escaped_tag = re.escape(tag)
                    tag_parts = tag.split(':')
                    
                    # íƒœê·¸ ì •ê·œì‹ ë§Œë“¤ê¸°
                    if len(tag_parts) > 1 and tag_parts[0] in namespaces:
                        # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ê°€ ìˆëŠ” ê²½ìš°
                        ns_prefix = tag_parts[0]
                        local_name = tag_parts[1]
                        
                        # ì¼ë°˜ì ì¸ íƒœê·¸ í˜•ì‹ (ë„¤ì„ìŠ¤í˜ì´ìŠ¤:ë¡œì»¬ëª…)
                        tag_pattern = escaped_tag
                        
                        # ëª¨ë“  contextRefì— ëŒ€í•´ ê²€ìƒ‰
                        for context_ref in matching_contexts:
                            for tag_format in [
                                # ì¼ë°˜ íƒœê·¸ í˜•ì‹
                                f'<{escaped_tag}\\s+[^>]*?contextRef="{re.escape(context_ref)}"[^>]*?>\\s*([^<]+?)\\s*</{escaped_tag}>',
                                # ì§§ì€ í˜•ì‹ (ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì—†ì´)
                                f'<{re.escape(local_name)}\\s+[^>]*?contextRef="{re.escape(context_ref)}"[^>]*?>\\s*([^<]+?)\\s*</{re.escape(local_name)}>',
                                # ì—­ë°©í–¥ íƒœê·¸ í˜•ì‹
                                f'</{escaped_tag}>\\s*([^<]+?)\\s*<{escaped_tag}\\s+[^>]*?contextRef="{re.escape(context_ref)}"[^>]*?>'
                            ]:
                                matches = re.findall(tag_format, xbrl_content)
                                if matches:
                                    # ìˆ«ì ê°’ í˜•ì‹í™” ì‹œë„
                                    raw_value = matches[0]
                                    
                                    # ë‹¨ìœ„ ì†ì„± ì¶”ì¶œ ì‹œë„
                                    decimals_match = re.search(f'<{escaped_tag}\\s+[^>]*?decimals="([^"]+)"[^>]*?>', xbrl_content)
                                    decimals_value = decimals_match.group(1) if decimals_match else "0"
                                    
                                    # ìˆ«ì í˜•ì‹ ë³€í™˜
                                    item_value = format_numeric_value(raw_value, decimals_value)
                                    found = True
                                    break
                            
                            if found:
                                break
                        
                    if found:
                        break
                
                if found:
                    break
            
            # ê²°ê³¼ì— í•­ëª© ì¶”ê°€ (ê°’ì„ ì°¾ì€ ê²½ìš°ì—ë§Œ ì‹¤ì œ ê°’, ì•„ë‹ˆë©´ N/A)
            result[item_name] = item_value
        
        return result
        
    except Exception as e:
        logger.error(f"XBRL íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
        return {key: "XBRL íŒŒì‹± ì˜¤ë¥˜" for key in items_and_tags.keys()}


def extract_business_section(document_text: str, section_type: str) -> str:
    """
    ê³µì‹œì„œë¥˜ ì›ë³¸íŒŒì¼ í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì • ë¹„ì¦ˆë‹ˆìŠ¤ ì„¹ì…˜ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        document_text: ê³µì‹œì„œë¥˜ ì›ë³¸ í…ìŠ¤íŠ¸
        section_type: ì¶”ì¶œí•  ì„¹ì…˜ ìœ í˜• 
                     ('ì‚¬ì—…ì˜ ê°œìš”', 'ì£¼ìš” ì œí’ˆ ë° ì„œë¹„ìŠ¤', 'ì›ì¬ë£Œ ë° ìƒì‚°ì„¤ë¹„',
                      'ë§¤ì¶œ ë° ìˆ˜ì£¼ìƒí™©', 'ìœ„í—˜ê´€ë¦¬ ë° íŒŒìƒê±°ë˜', 'ì£¼ìš”ê³„ì•½ ë° ì—°êµ¬ê°œë°œí™œë™',
                      'ê¸°íƒ€ ì°¸ê³ ì‚¬í•­')
    
    Returns:
        ì¶”ì¶œëœ ì„¹ì…˜ í…ìŠ¤íŠ¸ (íƒœê·¸ ì œê±° ë° ì •ë¦¬ëœ ìƒíƒœ)
    """
    import re
    
    # SECTION íƒœê·¸ í˜•ì‹ í™•ì¸
    section_tags = re.findall(r'<SECTION[^>]*>', document_text)
    section_end_tags = re.findall(r'</SECTION[^>]*>', document_text)
    
    # TITLE íƒœê·¸ê°€ ìˆëŠ”ì§€ í™•ì¸
    title_tags = re.findall(r'<TITLE[^>]*>(.*?)</TITLE>', document_text)
    
    # ì„¹ì…˜ íƒ€ì…ë³„ íŒ¨í„´ ë§¤í•‘ (ë²ˆí˜¸ê°€ í¬í•¨ëœ ê²½ìš°ë„ ì²˜ë¦¬) - lookahead êµ¬ë¬¸ ìˆ˜ì •
    section_patterns = {
        'ì‚¬ì—…ì˜ ê°œìš”': r'<TITLE[^>]*>(?:\d+\.\s*)?ì‚¬ì—…ì˜\s*ê°œìš”[^<]*</TITLE>(.*?)(?:(?=<TITLE)|(?=</SECTION))',
        'ì£¼ìš” ì œí’ˆ ë° ì„œë¹„ìŠ¤': r'<TITLE[^>]*>(?:\d+\.\s*)?ì£¼ìš”\s*ì œí’ˆ[^<]*</TITLE>(.*?)(?:(?=<TITLE)|(?=</SECTION))',
        'ì›ì¬ë£Œ ë° ìƒì‚°ì„¤ë¹„': r'<TITLE[^>]*>(?:\d+\.\s*)?ì›ì¬ë£Œ[^<]*</TITLE>(.*?)(?:(?=<TITLE)|(?=</SECTION))',
        'ë§¤ì¶œ ë° ìˆ˜ì£¼ìƒí™©': r'<TITLE[^>]*>(?:\d+\.\s*)?ë§¤ì¶œ[^<]*</TITLE>(.*?)(?:(?=<TITLE)|(?=</SECTION))',
        'ìœ„í—˜ê´€ë¦¬ ë° íŒŒìƒê±°ë˜': r'<TITLE[^>]*>(?:\d+\.\s*)?ìœ„í—˜ê´€ë¦¬[^<]*</TITLE>(.*?)(?:(?=<TITLE)|(?=</SECTION))',
        'ì£¼ìš”ê³„ì•½ ë° ì—°êµ¬ê°œë°œí™œë™': r'<TITLE[^>]*>(?:\d+\.\s*)?ì£¼ìš”\s*ê³„ì•½[^<]*</TITLE>(.*?)(?:(?=<TITLE)|(?=</SECTION))',
        'ê¸°íƒ€ ì°¸ê³ ì‚¬í•­': r'<TITLE[^>]*>(?:\d+\.\s*)?ê¸°íƒ€\s*ì°¸ê³ ì‚¬í•­[^<]*</TITLE>(.*?)(?:(?=<TITLE)|(?=</SECTION))',
    }
    
    # ìš”ì²­ëœ ì„¹ì…˜ íŒ¨í„´ í™•ì¸
    if section_type not in section_patterns:
        return f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„¹ì…˜ ìœ í˜•ì…ë‹ˆë‹¤. ì§€ì›ë˜ëŠ” ìœ í˜•: {', '.join(section_patterns.keys())}"
    
    # í•´ë‹¹ ì„¹ì…˜ê³¼ ì¼ì¹˜í•˜ëŠ” ì œëª© ì°¾ê¸°
    section_keyword = section_type.split(' ')[0]
    matching_titles = [title for title in title_tags if section_keyword.lower() in title.lower()]
    
    # ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ìœ¼ë¡œ ì„¹ì…˜ ì¶”ì¶œ ì‹œë„ 1: ê¸°ë³¸ íŒ¨í„´
    pattern = section_patterns[section_type]
    matches = re.search(pattern, document_text, re.DOTALL | re.IGNORECASE)
    
    # ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ìœ¼ë¡œ ì„¹ì…˜ ì¶”ì¶œ ì‹œë„ 2: SECTION íƒœê·¸ ì¢…ë£Œ íŒ¨í„´ ìˆ˜ì •
    if not matches:
        # SECTION-ìˆ«ì í˜•íƒœì˜ ì¢…ë£Œ íƒœê·¸ ì§€ì›
        pattern = section_patterns[section_type].replace('</SECTION', '</SECTION(?:-\\d+)?')
        matches = re.search(pattern, document_text, re.DOTALL | re.IGNORECASE)
    
    # ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ìœ¼ë¡œ ì„¹ì…˜ ì¶”ì¶œ ì‹œë„ 3: ê°œë³„ TITLE ì§ì ‘ ê²€ìƒ‰
    if not matches and matching_titles:
        for title in matching_titles:
            escaped_title = re.escape(title)
            direct_pattern = f'<TITLE[^>]*>{escaped_title}</TITLE>(.*?)(?=<TITLE|</SECTION(?:-\\d+)?)'
            matches = re.search(direct_pattern, document_text, re.DOTALL | re.IGNORECASE)
            if matches:
                break
    
    if not matches:
        return f"'{section_type}' ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # ì¶”ì¶œëœ í…ìŠ¤íŠ¸
    section_text = matches.group(1)
    
    # íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ë¦¬
    clean_text = re.sub(r'<[^>]*>', ' ', section_text)  # HTML íƒœê·¸ ì œê±°
    clean_text = re.sub(r'USERMARK\s*=\s*"[^"]*"', '', clean_text)  # USERMARK ì œê±°
    clean_text = re.sub(r'\s+', ' ', clean_text)  # ì—°ì†ëœ ê³µë°± ì œê±°
    clean_text = re.sub(r'\n\s*\n', '\n\n', clean_text)  # ë¹ˆ ì¤„ ì²˜ë¦¬
    clean_text = clean_text.strip()  # ì•ë’¤ ê³µë°± ì œê±°
    
    return clean_text


async def get_original_document(rcept_no: str) -> Tuple[str, Optional[bytes]]:
    """
    DART ê³µì‹œì„œë¥˜ ì›ë³¸íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        rcept_no: ê³µì‹œ ì ‘ìˆ˜ë²ˆí˜¸(14ìë¦¬)
        
    Returns:
        (íŒŒì¼ ë‚´ìš© ë¬¸ìì—´ ë˜ëŠ” ì˜¤ë¥˜ ë©”ì‹œì§€, ì›ë³¸ ë°”ì´ë„ˆë¦¬ ë°ì´í„°(ì„±ê³µ ì‹œ) ë˜ëŠ” None(ì‹¤íŒ¨ ì‹œ))
    """
    url = f"{BASE_URL}/document.xml?crtfc_key={API_KEY}&rcept_no={rcept_no}"
    
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(url)
            
            # ì‘ë‹µ ë°ì´í„° ê¸°ë³¸ ì •ë³´ ë¡œê¹…
            content_type = response.headers.get('content-type', 'ì•Œ ìˆ˜ ì—†ìŒ')
            content_length = len(response.content)
            content_md5 = hashlib.md5(response.content).hexdigest()
            
            logger.info(f"DART API ì›ë³¸ ë¬¸ì„œ ì‘ë‹µ ì •ë³´: URL={url}, ìƒíƒœì½”ë“œ={response.status_code}, Content-Type={content_type}, í¬ê¸°={content_length}ë°”ì´íŠ¸, MD5={content_md5}")
            
            if response.status_code != 200:
                logger.error(f"ì›ë³¸ ë¬¸ì„œ API ìš”ì²­ ì‹¤íŒ¨: HTTP ìƒíƒœ ì½”ë“œ {response.status_code}")
                return f"API ìš”ì²­ ì‹¤íŒ¨: HTTP ìƒíƒœ ì½”ë“œ {response.status_code}", None
            
            # API ì˜¤ë¥˜ ë©”ì‹œì§€ í™•ì¸ ì‹œë„ (XML í˜•ì‹ì¼ ìˆ˜ ìˆìŒ)
            try:
                root = ET.fromstring(response.content)
                status = root.findtext('status')
                message = root.findtext('message')
                if status and message:
                    logger.error(f"DART API ì˜¤ë¥˜ ì‘ë‹µ: status={status}, message={message}")
                    return f"DART API ì˜¤ë¥˜: {status} - {message}", None
            except ET.ParseError:
                # íŒŒì‹± ì˜¤ë¥˜ëŠ” ì •ìƒì ì¸ ZIP íŒŒì¼ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê³„ì† ì§„í–‰
                pass
            
            try:
                # ZIP íŒŒì¼ ì²˜ë¦¬
                with zipfile.ZipFile(BytesIO(response.content)) as zip_file:
                    # ì••ì¶• íŒŒì¼ ë‚´ì˜ íŒŒì¼ ëª©ë¡
                    file_list = zip_file.namelist()
                    
                    logger.info(f"ZIP íŒŒì¼ ë‚´ íŒŒì¼ ëª©ë¡: {file_list}")
                    
                    if not file_list:
                        return "ZIP íŒŒì¼ ë‚´ì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.", None
                    
                    # íŒŒì¼ëª…ì´ ê°€ì¥ ì§§ì€ íŒŒì¼ ì„ íƒ (ì¼ë°˜ì ìœ¼ë¡œ ë©”ì¸ íŒŒì¼ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ)
                    target_file = min(file_list, key=len)
                    file_ext = target_file.split('.')[-1].lower()
                    
                    logger.info(f"ì„ íƒëœ ëŒ€ìƒ íŒŒì¼: {target_file}, í™•ì¥ì: {file_ext}")
                    
                    # íŒŒì¼ ë‚´ìš© ì½ê¸°
                    with zip_file.open(target_file) as doc_file:
                        file_content = doc_file.read()
                        
                        # í…ìŠ¤íŠ¸ íŒŒì¼ì¸ ê²½ìš° (txt, html, xml ë“±)
                        if file_ext in ['txt', 'html', 'htm', 'xml', 'xbrl']:
                            # ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„
                            encodings = ['utf-8', 'euc-kr', 'cp949']
                            text_content = None
                            
                            for encoding in encodings:
                                try:
                                    text_content = file_content.decode(encoding)
                                    logger.info(f"íŒŒì¼ {target_file} ë””ì½”ë”© ì„±ê³µ (ì¸ì½”ë”©: {encoding})")
                                    break
                                except UnicodeDecodeError:
                                    logger.info(f"íŒŒì¼ {target_file} {encoding} ë””ì½”ë”© ì‹¤íŒ¨")
                                    continue
                            
                            if text_content:
                                return text_content, file_content
                            else:
                                logger.error(f"íŒŒì¼ {target_file} ëª¨ë“  ì¸ì½”ë”© ë””ì½”ë”© ì‹¤íŒ¨")
                                return "íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ì¸ì½”ë”© ë¬¸ì œ).", file_content
                        # PDF ë˜ëŠ” ê¸°íƒ€ ë°”ì´ë„ˆë¦¬ íŒŒì¼
                        else:
                            logger.info(f"ë¹„í…ìŠ¤íŠ¸ íŒŒì¼ í˜•ì‹ ê°ì§€: {file_ext}")
                            return f"íŒŒì¼ì´ í…ìŠ¤íŠ¸ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤ (í˜•ì‹: {file_ext}).", file_content
                        
            except zipfile.BadZipFile:
                logger.error(f"ìœ íš¨í•˜ì§€ ì•Šì€ ZIP íŒŒì¼: URL={url}, Content-Type={content_type}, í¬ê¸°={content_length}ë°”ì´íŠ¸")
                
                # íŒŒì¼ ì‹œì‘ ë¶€ë¶„(ì²˜ìŒ 50~100ë°”ì´íŠ¸) 16ì§„ìˆ˜ë¡œ ë¤í”„í•˜ì—¬ ë¡œê¹…
                content_head = response.content[:100]
                hex_dump = binascii.hexlify(content_head).decode('utf-8')
                hex_formatted = ' '.join(hex_dump[i:i+2] for i in range(0, len(hex_dump), 2))
                logger.error(f"ìœ íš¨í•˜ì§€ ì•Šì€ ZIP íŒŒì¼ í—¤ë” ë¤í”„(100ë°”ì´íŠ¸): {hex_formatted}")
                
                # ì—¬ëŸ¬ ì¸ì½”ë”©ìœ¼ë¡œ í•´ì„ ì‹œë„í•˜ê³  ë‚´ìš© ë¡œê¹…
                encodings_to_try = ['utf-8', 'euc-kr', 'cp949', 'latin-1']
                for encoding in encodings_to_try:
                    try:
                        content_preview = response.content[:1000].decode(encoding)
                        content_preview = content_preview.replace('\n', ' ')[:200]  # ì¤„ë°”ê¿ˆ ì œê±°, 200ìë¡œ ì œí•œ
                        logger.info(f"{encoding} ì¸ì½”ë”©ìœ¼ë¡œ í•´ì„í•œ ë‚´ìš©(ì¼ë¶€): {content_preview}")
                        
                        # XML í˜•ì‹ì¸ì§€ í™•ì¸
                        if content_preview.strip().startswith('<?xml') or content_preview.strip().startswith('<'):
                            logger.info(f"ì‘ë‹µ ë°ì´í„°ê°€ XML í˜•ì‹ì¼ ê°€ëŠ¥ì„±ì´ ìˆìŒ (ì¸ì½”ë”©: {encoding})")
                            try:
                                error_root = ET.fromstring(response.content.decode(encoding))
                                error_status = error_root.findtext('status')
                                error_message = error_root.findtext('message')
                                if error_status and error_message:
                                    logger.info(f"ì˜¤ë¥˜ XML íŒŒì‹± ì„±ê³µ: {error_status} - {error_message}")
                                    return f"DART API ì˜¤ë¥˜: {error_status} - {error_message}", None
                            except ET.ParseError as xml_err:
                                logger.error(f"XML íŒŒì‹± ì‹œë„ ì‹¤íŒ¨: {xml_err}")
                            
                    except UnicodeDecodeError:
                        logger.info(f"{encoding} ì¸ì½”ë”©ìœ¼ë¡œ í•´ì„ ì‹¤íŒ¨")
                
                # íŒŒì¼ ì‹œê·¸ë‹ˆì²˜ í™•ì¸ (ì²˜ìŒ 4~8ë°”ì´íŠ¸)
                file_sig_hex = binascii.hexlify(response.content[:8]).decode('utf-8')
                logger.info(f"íŒŒì¼ ì‹œê·¸ë‹ˆì²˜(HEX): {file_sig_hex}")
                
                # ì¼ë°˜ì ì¸ íŒŒì¼ í˜•ì‹ë“¤ì˜ ì‹œê·¸ë‹ˆì²˜ì™€ ë¹„êµ
                known_signatures = {
                    "504b0304": "ZIP íŒŒì¼(ì •ìƒ)",
                    "3c3f786d": "XML ë¬¸ì„œ",
                    "7b227374": "JSON ë¬¸ì„œ",
                    "1f8b0800": "GZIP ì••ì¶•íŒŒì¼",
                    "ffd8ffe0": "JPEG ì´ë¯¸ì§€",
                    "89504e47": "PNG ì´ë¯¸ì§€",
                    "25504446": "PDF ë¬¸ì„œ"
                }
                
                for sig, desc in known_signatures.items():
                    if file_sig_hex.startswith(sig):
                        logger.info(f"íŒŒì¼ í˜•ì‹ ì¸ì‹: {desc}")
                
                return "ë‹¤ìš´ë¡œë“œí•œ íŒŒì¼ì´ ìœ íš¨í•œ ZIP íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤.", None
                
    except httpx.RequestError as e:
        logger.error(f"ì›ë³¸ ë¬¸ì„œ API ìš”ì²­ ì¤‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
        return f"API ìš”ì²­ ì¤‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", None
    except Exception as e:
        logger.error(f"ê³µì‹œ ì›ë³¸ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}, ìŠ¤íƒíŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        return f"ê³µì‹œ ì›ë³¸ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", None


async def extract_business_section_from_dart(rcept_no: str, section_type: str) -> str:
    """
    DART APIë¥¼ í†µí•´ ê³µì‹œì„œë¥˜ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  íŠ¹ì • ë¹„ì¦ˆë‹ˆìŠ¤ ì„¹ì…˜ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        rcept_no: ê³µì‹œ ì ‘ìˆ˜ë²ˆí˜¸(14ìë¦¬)
        section_type: ì¶”ì¶œí•  ì„¹ì…˜ ìœ í˜• 
                    ('ì‚¬ì—…ì˜ ê°œìš”', 'ì£¼ìš” ì œí’ˆ ë° ì„œë¹„ìŠ¤', 'ì›ì¬ë£Œ ë° ìƒì‚°ì„¤ë¹„',
                     'ë§¤ì¶œ ë° ìˆ˜ì£¼ìƒí™©', 'ìœ„í—˜ê´€ë¦¬ ë° íŒŒìƒê±°ë˜', 'ì£¼ìš”ê³„ì•½ ë° ì—°êµ¬ê°œë°œí™œë™',
                     'ê¸°íƒ€ ì°¸ê³ ì‚¬í•­')
    
    Returns:
        ì¶”ì¶œëœ ì„¹ì…˜ í…ìŠ¤íŠ¸ ë˜ëŠ” ì˜¤ë¥˜ ë©”ì‹œì§€
    """
    # ì›ë³¸ ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ
    document_text, binary_data = await get_original_document(rcept_no)
    
    # ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ì‹œ
    if binary_data is None:
        return f"ê³µì‹œì„œë¥˜ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {document_text}"
    
    # ì„¹ì…˜ ì¶”ì¶œ
    section_text = extract_business_section(document_text, section_type)
    
    return section_text

def detect_encoding(response):
    """ì‘ë‹µì—ì„œ ì¸ì½”ë”©ì„ ê°ì§€í•©ë‹ˆë‹¤."""
    # 1. Content-Type í—¤ë”ì—ì„œ ì¸ì½”ë”© í™•ì¸
    content_type = response.headers.get('content-type', '')
    if 'charset=' in content_type:
        charset = content_type.split('charset=')[-1].split(';')[0].strip()
        return charset
        
    # 2. ì‘ë‹µ ì½˜í…ì¸ ì—ì„œ ì¸ì½”ë”© ê°ì§€
    content = response.content
    if content:
        encoding = chardet.detect(content)['encoding']
        if encoding:
            return encoding
    
    # 3. ê¸°ë³¸ ì¸ì½”ë”© ë°˜í™˜
    return 'utf-8'

def extract_content_from_html(html: str, url: str) -> Dict[str, Any]:
    """HTMLì—ì„œ ì£¼ìš” ë‚´ìš©ì„ ì¶”ì¶œí•˜ê³  ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    soup = BeautifulSoup(html, 'html.parser', from_encoding='utf-8')
    
    # ë©”ì¸ ì½˜í…ì¸  ì¶”ì¶œ ì‹œë„
    main_content = soup.find('main') or soup.find('article') or soup.find('div', class_='content')
    
    if not main_content:
        main_content = soup.body
    
    # ì´ë¯¸ì§€ ì¶”ì¶œ
    images = []
    for img in main_content.find_all('img'):
        src = img.get('src', '')
        alt = img.get('alt', '')
        if src:
            images.append({"src": src, "alt": alt})
    
    # HTMLì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
    markdown = markdownify.markdownify(str(main_content))
    
    return {
        "markdown": markdown,
        "images": images
    }

async def _fetch_url_content(url: str, force_raw: bool = False) -> Dict[str, Any]:
    """URLì„ ê°€ì ¸ì™€ ì½˜í…ì¸ ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        async with httpx.AsyncClient(timeout=15.0) as client:
            response = await client.get(
                url, 
                follow_redirects=True,
                headers={"User-Agent": DEFAULT_USER_AGENT}
            )
            response.raise_for_status()
            
            # ì¸ì½”ë”© ê°ì§€ ë° ì„¤ì •
            encoding = detect_encoding(response)
            response.headers.encoding = encoding
            
            content_type = response.headers.get("content-type", "").lower()
            text = response.text
            
            is_html = "<html" in text.lower() or "text/html" in content_type
            
            if is_html and not force_raw:
                try:
                    result = extract_content_from_html(text, url)
                    markdown = result["markdown"]
                    images = result["images"]
                    image_urls = [img["src"] for img in images]
                    
                    return {
                        "content": markdown,
                        "prefix": "",
                        "image_urls": image_urls,
                        "success": True
                    }
                except Exception as e:
                    logger.warning(f"HTML ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}, ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜")
                    return {
                        "content": text,
                        "prefix": "HTML ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì›ë³¸ ë‚´ìš©:\n",
                        "image_urls": [],
                        "success": True
                    }
            
            # JSON ë˜ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            if "application/json" in content_type:
                return {
                    "content": json.dumps(response.json(), ensure_ascii=False, indent=2),
                    "prefix": "JSON ì½˜í…ì¸ :\n",
                    "image_urls": [],
                    "success": True
                }
            else:
                return {
                    "content": text,
                    "prefix": f"ì½˜í…ì¸  íƒ€ì… {content_type}:\n",
                    "image_urls": [],
                    "success": True
                }
    except httpx.RequestError as e:
        return {
            "content": f"URL ìš”ì²­ ì˜¤ë¥˜: {e}",
            "prefix": "ì˜¤ë¥˜ ë°œìƒ:\n",
            "image_urls": [],
            "success": False
        }
    except httpx.HTTPStatusError as e:
        return {
            "content": f"HTTP ìƒíƒœ ì˜¤ë¥˜: {e.response.status_code} - {e}",
            "prefix": "ì˜¤ë¥˜ ë°œìƒ:\n",
            "image_urls": [],
            "success": False
        }
    except Exception as e:
        return {
            "content": f"ì˜¤ë¥˜ ë°œìƒ: {e}",
            "prefix": "ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜:\n",
            "image_urls": [],
            "success": False
        }

# --- ê¸°ì¡´ fetch_url ë„êµ¬ êµì²´ ---
@mcp.tool(
    name="fetch_url",
    description="Fetches the content of a given URL. Attempts to convert HTML to markdown and extract images."
)
async def fetch_url(url: str, max_length: int = 20000, start_index: int = 0, raw: bool = False) -> str:
    """
    URL ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸° ë„êµ¬
    
    ì£¼ì–´ì§„ URLì˜ ë‚´ìš©ì„ ê°€ì ¸ì™€ HTMLì¸ ê²½ìš° ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì´ë¯¸ì§€ URLë„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        url: ê°€ì ¸ì˜¬ URL
        max_length: ë°˜í™˜í•  ìµœëŒ€ ì½˜í…ì¸  ê¸¸ì´ (ê¸°ë³¸ê°’: 20000)
        start_index: ì½˜í…ì¸  ì‹œì‘ ì¸ë±ìŠ¤ (ê¸°ë³¸ê°’: 0)
        raw: HTMLì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜í•˜ì§€ ì•Šê³  ì›ì‹œ ë‚´ìš©ì„ ë°˜í™˜í• ì§€ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
        
    Returns:
        str: ë³€í™˜ëœ ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸  ë˜ëŠ” ì›ì‹œ ì½˜í…ì¸ , ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜¤ë¥˜ ë©”ì‹œì§€
    """
    try:
        result = await _fetch_url_content(url, raw)
        content = result["content"]
        prefix = result["prefix"]
        image_urls = result.get("image_urls", [])
        success = result.get("success", False)
        
        # ì½˜í…ì¸  ê¸¸ì´ ì œí•œ
        final_content = content
        if len(final_content) > max_length:
            final_content = final_content[start_index:start_index + max_length]
            final_content += f"\n\n<ì¶”ê°€ ë‚´ìš©ì´ ì˜ë ¸ìŠµë‹ˆë‹¤. ë” ë§ì€ ë‚´ìš©ì„ ë³´ë ¤ë©´ start_index={start_index + max_length}ë¡œ ë‹¤ì‹œ í˜¸ì¶œí•˜ì„¸ìš”.>"
        
        # ì´ë¯¸ì§€ ì„¹ì…˜ ì¶”ê°€
        images_section = ""
        if image_urls and len(image_urls) > 0:
            images_section = "\n\në°œê²¬ëœ ì´ë¯¸ì§€:\n" + "\n".join([f"- {img}" for img in image_urls])
        
        if not success:
            return f"{prefix}{final_content}"
        else:
            return f"{prefix}{url} ë‚´ìš©:\n\n{final_content}{images_section}"
    
    except Exception as error:
        return f"URL ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {str(error)}"

# --- êµ¬ê¸€ ê²€ìƒ‰ MCP ë„êµ¬ ---
@mcp.tool()
async def search_google(query: str, num_results: int = 10) -> str:
    """
    êµ¬ê¸€ ê²€ìƒ‰ ë„êµ¬
    
    êµ¬ê¸€ ë§ì¶¤ ê²€ìƒ‰ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        query: ê²€ìƒ‰ì–´
        num_results: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 10)
        
    Returns:
        str: ê²€ìƒ‰ ê²°ê³¼ ë¬¸ìì—´
    """
    try:
        # Google API í‚¤ ë° CSE ID í™•ì¸
        if not GOOGLE_API_KEY or not GOOGLE_CSE_ID:
            return "êµ¬ê¸€ ê²€ìƒ‰ APIê°€ êµ¬ì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GOOGLE_API_KEYì™€ GOOGLE_CSE_ID í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”."
        
        # êµ¬ê¸€ ë§ì¶¤ ê²€ìƒ‰ API ì´ˆê¸°í™”
        service = build("customsearch", "v1", developerKey=GOOGLE_API_KEY)
        
        # ê²€ìƒ‰ ì‹¤í–‰
        result = service.cse().list(
            q=query,
            cx=GOOGLE_CSE_ID,
            num=num_results
        ).execute()
        
        # ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ…
        formatted_results = []
        if "items" in result:
            for item in result["items"]:
                formatted_results.append({
                    "title": item.get("title", ""),
                    "link": item.get("link", ""),
                    "snippet": item.get("snippet", "")
                })
        
        # ê²°ê³¼ ë¬¸ìì—´ êµ¬ì„±
        total_results = result.get("searchInformation", {}).get("totalResults", "0")
        response_text = f"êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ (ì´ {total_results}ê±´):\n\n"
        
        for i, item in enumerate(formatted_results, 1):
            response_text += f"### ê²°ê³¼ {i}\n"
            response_text += f"ì œëª©: {item['title']}\n"
            response_text += f"ë§í¬: {item['link']}\n"
            response_text += f"ì„¤ëª…: {item['snippet']}\n\n"
        
        if not formatted_results:
            response_text += "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        return response_text
    
    except HttpError as error:
        return f"API ì˜¤ë¥˜: {str(error)}"
    except Exception as error:
        return f"ì˜¤ë¥˜ ë°œìƒ: {str(error)}"

# --- ë©”ì¸ ì‹¤í–‰ ì½”ë“œ ---
if __name__ == "__main__":
    # API í‚¤ í™•ì¸
    if not API_KEY:
        logger.warning("DART_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. DART API ê¸°ëŠ¥ì´ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        logger.warning("NAVER_CLIENT_ID ë˜ëŠ” NAVER_CLIENT_SECRET í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë„¤ì´ë²„ ê²€ìƒ‰ ê¸°ëŠ¥ì´ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    if not GOOGLE_API_KEY or not GOOGLE_CSE_ID:
        logger.warning("GOOGLE_API_KEY ë˜ëŠ” GOOGLE_CSE_ID í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. êµ¬ê¸€ ê²€ìƒ‰ ê¸°ëŠ¥ì´ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # ì„œë²„ ì‹¤í–‰
    logger.info("Company Analysis MCP Server ì‹œì‘ ì¤‘...")
    mcp.run()
